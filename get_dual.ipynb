{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16439587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import numpy as np\n",
    "import torch\n",
    "ora_W = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/wanda/90/8_self_attn.v_proj_ora_W.npy')\n",
    "new_W = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/wanda/90/8_self_attn.v_proj_new_W.npy')\n",
    "test_inps = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/wanda/90/8_self_attn.v_proj_test_inps.npy')\n",
    "test_outs = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/wanda/90/8_self_attn.v_proj_test_outs.npy')\n",
    "\n",
    "ora_W = torch.tensor(ora_W).to(torch.float16).cuda()\n",
    "new_W = torch.tensor(new_W).to(torch.float16).cuda()\n",
    "test_inps = torch.tensor(test_inps).to(torch.float16)\n",
    "test_outs = torch.tensor(test_outs).to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da461ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9000, device='cuda:0')\n",
      "tensor(1.8311e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print((new_W == 0).sum()/ new_W.numel())\n",
    "print((ora_W == 0).sum()/ ora_W.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b528a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.556152\n",
      "Error: 0.556152\n"
     ]
    }
   ],
   "source": [
    "def get_error(ora_W, new_W, test_inps, test_outs):\n",
    "    results = 0\n",
    "    for i in range(test_inps.shape[0]):\n",
    "        inp = test_inps[i].unsqueeze(0).cuda()\n",
    "        out = test_outs[i].unsqueeze(0).cuda()\n",
    "        result = torch.norm((new_W @ inp) - (out)) / torch.norm(out)\n",
    "        results += result.item()\n",
    "    results /= test_inps.shape[0]\n",
    "    return result\n",
    "\n",
    "init_W = ora_W.clone()\n",
    "init_W[new_W == 0] = 0\n",
    "\n",
    "\n",
    "error = get_error(ora_W, init_W, test_inps, test_outs)\n",
    "print(f\"Error: {error:.6f}\")\n",
    "\n",
    "error = get_error(ora_W, new_W, test_inps, test_outs)\n",
    "print(f\"Error: {error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78abfdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.1000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 44.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 4.537440299987793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def delta_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end - alpha_start\n",
    "    else:\n",
    "        return alpha_schedule_exp(k, max_iter, alpha_start, alpha_end, gamma) - alpha_start\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "    A_start = torch.linalg.cholesky(A)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    \n",
    "    A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "    A2 = (-A1) @ (I - H_A) @ A_start\n",
    "    A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "    alpha_start = alpha\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha_start)\n",
    "        delta = alpha - alpha_start\n",
    "\n",
    "        if k % 20 == 0 and k < 100:\n",
    "            alpha_start = alpha\n",
    "\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            \n",
    "            A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "            A2 = (-A1) @ (I - H_A) @ A_start\n",
    "            A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "\n",
    "        # A_inv = (1-alpha-delta) * H_A + (alpha + delta) * I\n",
    "        # A_inv = torch.linalg.cholesky(A_inv)\n",
    "        # A_inv = torch.cholesky_inverse(A_inv)\n",
    "\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2 + delta**3 * A3)\n",
    "        # W = B @ A_inv\n",
    "\n",
    "        # print(\"二阶近似误差\",torch.norm(((A_start + delta * A1 + delta**2 * A2) - A_inv)))\n",
    "\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=200)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c39f30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.151367\n"
     ]
    }
   ],
   "source": [
    "error = get_error(ora_W, W_update, test_inps, test_outs)\n",
    "print(f\"Final Error: {error:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
