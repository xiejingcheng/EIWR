{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f412dbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 4,5,6,7\n",
      "torch 2.3.1+cu121\n",
      "torch 2.3.1+cu121\n",
      "transformers 4.47.1\n",
      "accelerate 0.29.1\n",
      "# of gpus:  4\n",
      "torch 2.3.1+cu121\n",
      "transformers 4.47.1\n",
      "accelerate 0.29.1\n",
      "# of gpus:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 17:00:15.159836: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-25 17:00:15.319573: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-25 17:00:15.324427: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-05-25 17:00:15.324447: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-05-25 17:00:16.914116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-05-25 17:00:16.914245: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-05-25 17:00:16.914257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be9ac7420bf47ebab168a7c95fb3893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 5120])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.0.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.1.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.2.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.3.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.4.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.5.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.6.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.7.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.8.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.9.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.10.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.11.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.12.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.13.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.14.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.15.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.16.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.17.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.18.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.19.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.20.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.21.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.22.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.23.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.24.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.25.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.26.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.27.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.28.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.29.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.30.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.31.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.32.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.33.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.34.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.35.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.36.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.37.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.38.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.39.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.norm.weight torch.Size([5120])\n",
      "lm_head.weight torch.Size([32000, 5120])\n",
      "Starting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "100%|██████████| 128/128 [00:50<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# from matmul_had import *\n",
    "# from utils import *\n",
    "import os\n",
    "\n",
    "# 设置只使用特定的 GPU（例如 GPU 0 和 GPU 1）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'\n",
    "\n",
    "# 检查设置是否生效\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "import argparse\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from importlib.metadata import version\n",
    "print('torch', version('torch'))\n",
    "\n",
    "from lib.utils import find_layers\n",
    "from lib.data import prepare_calibration_input\n",
    "from lib.data import get_loaders \n",
    "from lib.eval import eval_ppl, eval_zero_shot\n",
    "from lib.esd_utils import get_esd_metrics\n",
    "\n",
    "from main import get_llm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('torch', version('torch'))\n",
    "print('transformers', version('transformers'))\n",
    "print('accelerate', version('accelerate'))\n",
    "print('# of gpus: ', torch.cuda.device_count())\n",
    "\n",
    "class PruneConfig:\n",
    "    def __init__(self):\n",
    "        self.model = '/h3cstore_ns/jcxie/hf_weights/llama-2-13b-hf'\n",
    "        self.seed = 0\n",
    "        self.nsamples = 128\n",
    "        self.sparsity_ratio = 0.7\n",
    "        self.sparsity_type = \"unstructured\"\n",
    "        self.prune_method = \"sparsegpt_silu_ww\"\n",
    "        self.cache_dir = \"llm_weights\"\n",
    "        self.use_variant = False\n",
    "        self.save = '/h3cstore_ns/jcxie/LISA/wanda-main/ckpt'\n",
    "        self.save_model = None\n",
    "        self.exclude = 'gate_proj'\n",
    "        self.ww_metric = \"alpha_peak\"\n",
    "        self.ww_metric_cache = \"/h3cstore_ns/jcxie/LISA/wanda-main/data/llama2-7b-hf\"\n",
    "        self.epsilon = 0.3\n",
    "        self.mapping_type = \"block_wise\"\n",
    "        self.Hyper_m = 3.0\n",
    "        self.Lamda = 0.20\n",
    "        self.eval_zero_shot = False\n",
    "args = PruneConfig()\n",
    "model = get_llm(args.model, args.cache_dir)\n",
    "model.eval()\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.size())\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "    ratios = None\n",
    "    device = model.hf_device_map[\"lm_head\"]\n",
    "\n",
    "    # print(\"loading calibdation data\")\n",
    "    # dataloader, _ = get_loaders(\"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)\n",
    "    # print(\"dataset loading complete\")\n",
    "    # with torch.no_grad():\n",
    "    #     inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device, nsamples=args.nsamples)\n",
    "\n",
    "    layers = model.model.layers\n",
    "\n",
    "    layer_num = len(find_layers(layers))\n",
    "    if ratios is None:\n",
    "        ratios = [args.sparsity_ratio for i in range(layer_num)]\n",
    "    k=0\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "with torch.no_grad():\n",
    "    print('Starting ...')\n",
    "    dataloader, _ = get_loaders(args, \"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)\n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    if \"model.embed_tokens\" in model.hf_device_map:\n",
    "        dev = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros(\n",
    "        (args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
    "    )\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask']\n",
    "            cache['position_ids'] = kwargs['position_ids']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch[0].to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache['attention_mask']\n",
    "    position_ids = cache['position_ids']\n",
    "\n",
    "    print('Ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8baf312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 self_attn.q_proj\n",
      "Pruning ...\n",
      "0 self_attn.k_proj\n",
      "Pruning ...\n",
      "0 self_attn.v_proj\n",
      "Pruning ...\n",
      "0 self_attn.o_proj\n",
      "Pruning ...\n",
      "0 mlp.gate_proj\n",
      "Pruning ...\n",
      "0 mlp.up_proj\n",
      "Pruning ...\n",
      "0 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 1 device 0\n",
      "1 self_attn.q_proj\n",
      "Pruning ...\n",
      "1 self_attn.k_proj\n",
      "Pruning ...\n",
      "1 self_attn.v_proj\n",
      "Pruning ...\n",
      "1 self_attn.o_proj\n",
      "Pruning ...\n",
      "1 mlp.gate_proj\n",
      "Pruning ...\n",
      "1 mlp.up_proj\n",
      "Pruning ...\n",
      "1 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 2 device 0\n",
      "2 self_attn.q_proj\n",
      "Pruning ...\n",
      "2 self_attn.k_proj\n",
      "Pruning ...\n",
      "2 self_attn.v_proj\n",
      "Pruning ...\n",
      "2 self_attn.o_proj\n",
      "Pruning ...\n",
      "2 mlp.gate_proj\n",
      "Pruning ...\n",
      "2 mlp.up_proj\n",
      "Pruning ...\n",
      "2 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 3 device 0\n",
      "3 self_attn.q_proj\n",
      "Pruning ...\n",
      "3 self_attn.k_proj\n",
      "Pruning ...\n",
      "3 self_attn.v_proj\n",
      "Pruning ...\n",
      "3 self_attn.o_proj\n",
      "Pruning ...\n",
      "3 mlp.gate_proj\n",
      "Pruning ...\n",
      "3 mlp.up_proj\n",
      "Pruning ...\n",
      "3 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 4 device 0\n",
      "4 self_attn.q_proj\n",
      "Pruning ...\n",
      "4 self_attn.k_proj\n",
      "Pruning ...\n",
      "4 self_attn.v_proj\n",
      "Pruning ...\n",
      "4 self_attn.o_proj\n",
      "Pruning ...\n",
      "4 mlp.gate_proj\n",
      "Pruning ...\n",
      "4 mlp.up_proj\n",
      "Pruning ...\n",
      "4 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 5 device 0\n",
      "5 self_attn.q_proj\n",
      "Pruning ...\n",
      "5 self_attn.k_proj\n",
      "Pruning ...\n",
      "5 self_attn.v_proj\n",
      "Pruning ...\n",
      "5 self_attn.o_proj\n",
      "Pruning ...\n",
      "5 mlp.gate_proj\n",
      "Pruning ...\n",
      "5 mlp.up_proj\n",
      "Pruning ...\n",
      "5 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 6 device 0\n",
      "6 self_attn.q_proj\n",
      "Pruning ...\n",
      "6 self_attn.k_proj\n",
      "Pruning ...\n",
      "6 self_attn.v_proj\n",
      "Pruning ...\n",
      "6 self_attn.o_proj\n",
      "Pruning ...\n",
      "6 mlp.gate_proj\n",
      "Pruning ...\n",
      "6 mlp.up_proj\n",
      "Pruning ...\n",
      "6 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 7 device 0\n",
      "7 self_attn.q_proj\n",
      "Pruning ...\n",
      "7 self_attn.k_proj\n",
      "Pruning ...\n",
      "7 self_attn.v_proj\n",
      "Pruning ...\n",
      "7 self_attn.o_proj\n",
      "Pruning ...\n",
      "7 mlp.gate_proj\n",
      "Pruning ...\n",
      "7 mlp.up_proj\n",
      "Pruning ...\n",
      "7 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 8 device 0\n",
      "8 self_attn.q_proj\n",
      "Pruning ...\n",
      "8 self_attn.k_proj\n",
      "Pruning ...\n",
      "8 self_attn.v_proj\n",
      "Pruning ...\n",
      "8 self_attn.o_proj\n",
      "Pruning ...\n",
      "8 mlp.gate_proj\n",
      "Pruning ...\n",
      "8 mlp.up_proj\n",
      "Pruning ...\n",
      "8 mlp.down_proj\n",
      "Pruning ...\n"
     ]
    }
   ],
   "source": [
    "prune_n, prune_m = 0, 0\n",
    "if args.sparsity_type != \"unstructured\":\n",
    "    assert args.sparsity_ratio == 0.5, \"sparsity ratio must be 0.5 for structured N:M sparsity\"\n",
    "    prune_n, prune_m = map(int, args.sparsity_type.split(\":\"))\n",
    "    \n",
    "from lib.sparsegpt import SparseGPTV2, SparseGPT\n",
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    \n",
    "    layer_num = len(find_layers(layers))\n",
    "    if ratios is None:\n",
    "        ratios = [args.sparsity_ratio for i in range(layer_num)]\n",
    "    k=0\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if f\"model.layers.{i}\" in model.hf_device_map:\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            print(f\"layer {i} device {dev}\")\n",
    "            if attention_mask is None:\n",
    "                inps, outs, position_ids = inps.to(dev), outs.to(dev), position_ids.to(dev)\n",
    "            else:\n",
    "                inps, outs, attention_mask, position_ids = inps.to(dev), outs.to(dev), attention_mask.to(dev), position_ids.to(dev)\n",
    "\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "\n",
    "            if name == 'self_attn.v_proj' and i == 8:\n",
    "                gpts[name] = SparseGPTV2(subset[name], have_scaler=True)\n",
    "            else:\n",
    "                gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "                if attention_mask is None:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), position_ids=position_ids)[0]\n",
    "                else:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        if i == 8:\n",
    "            test_inps = gpts['self_attn.v_proj'].inps\n",
    "            test_outs = gpts['self_attn.v_proj'].outs\n",
    "            ora_W = model.model.layers[i].self_attn.v_proj.weight.data\n",
    "            # np.save('/h3cstore_ns/jcxie/LISA/wanda-main/npys/test_admm/ora_W.npy', ora_W.cpu().numpy())\n",
    "\n",
    "        for name in gpts:\n",
    "            print(i, name)\n",
    "            print('Pruning ...')\n",
    "\n",
    "            gpts[name].fasterprune(ratios[k], prune_n=prune_n, prune_m=prune_m, percdamp=0.01, blocksize=128)\n",
    "            gpts[name].free()\n",
    "            k+=1\n",
    "        \n",
    "        if i == 8:\n",
    "            new_W = gpts['self_attn.v_proj'].layer.weight.data\n",
    "            scaler_row = gpts['self_attn.v_proj'].scaler_row\n",
    "            # np.save('/h3cstore_ns/jcxie/LISA/wanda-main/npys/test_admm/new_W.npy', new_W.cpu().numpy())\n",
    "            break\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            if attention_mask is None:\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), position_ids=position_ids)[0]\n",
    "            else:\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "\n",
    "        layers[i] = layer \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f50618",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inps = torch.stack(test_inps)\n",
    "test_outs = torch.stack(test_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2dd20a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ora_W torch.Size([5120, 5120])\n",
      "new_W torch.Size([5120, 5120])\n",
      "test_inps torch.Size([128, 5120, 4096])\n",
      "test_outs torch.Size([128, 1, 5120, 4096])\n"
     ]
    }
   ],
   "source": [
    "print('ora_W', ora_W.shape)\n",
    "print('new_W', new_W.shape)\n",
    "print('test_inps', test_inps.shape)\n",
    "print('test_outs', test_outs.shape)\n",
    "import torch\n",
    "ora_W = ora_W.cuda()\n",
    "new_W = new_W.cuda()\n",
    "test_inps = test_inps.cpu()\n",
    "test_outs = test_outs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f87887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_test_inps.npy', test_inps.cpu().numpy())\n",
    "print(1)\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_test_outs.npy', test_outs.cpu().numpy())\n",
    "print(2)\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_ora_W.npy', ora_W.cpu().numpy())\n",
    "print(3)\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_new_W.npy', new_W.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91ec625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "ora_W = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_ora_W.npy')\n",
    "new_W = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_new_W.npy')\n",
    "test_inps = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_test_inps.npy')\n",
    "test_outs = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_test_outs.npy')\n",
    "\n",
    "ora_W = torch.tensor(ora_W).to(torch.float16).cuda()\n",
    "new_W = torch.tensor(new_W).to(torch.float16).cuda()\n",
    "test_inps = torch.tensor(test_inps).to(torch.float16)\n",
    "test_outs = torch.tensor(test_outs).to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def delta_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end - alpha_start\n",
    "    else:\n",
    "        return alpha_schedule_exp(k, max_iter, alpha_start, alpha_end, gamma) - alpha_start\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "    A_start = torch.linalg.cholesky(A)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    \n",
    "    A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "    A2 = (-A1) @ (I - H_A) @ A_start\n",
    "    A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "    alpha_start = alpha\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha_start)\n",
    "        delta = alpha - alpha_start\n",
    "\n",
    "        if k % 20 == 0 and k < 100:\n",
    "            alpha_start = alpha\n",
    "\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            \n",
    "            A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "            A2 = (-A1) @ (I - H_A) @ A_start\n",
    "            A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "\n",
    "        # A_inv = (1-alpha-delta) * H_A + (alpha + delta) * I\n",
    "        # A_inv = torch.linalg.cholesky(A_inv)\n",
    "        # A_inv = torch.cholesky_inverse(A_inv)\n",
    "\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2 + delta**3 * A3)\n",
    "        # W = B @ A_inv\n",
    "\n",
    "        # print(\"二阶近似误差\",torch.norm(((A_start + delta * A1 + delta**2 * A2) - A_inv)))\n",
    "\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=1000)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e642ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.3000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [00:04<00:25, 33.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5815, device='cuda:0', dtype=torch.float16) tensor(0.0682, device='cuda:0', dtype=torch.float16)\n",
      "Converged at iteration 150\n",
      "tensor(0.0222, device='cuda:0')\n",
      "time 4.870270729064941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.5, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method8(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "\n",
    "    eigvals, Q = torch.linalg.eigh(H_A)\n",
    "\n",
    "    def apply_A_inv_right(X, eigvals, Q, alpha):\n",
    "        # X: (n, n)\n",
    "        d = 1.0 / ((1 - alpha) * eigvals + alpha)  # shape: (n,)\n",
    "        tmp = X @ Q                                # right multiply Q\n",
    "        tmp = tmp * d                              # element-wise scale columns\n",
    "        return tmp @ Q.T                           # right multiply Q^T\n",
    "    \n",
    "\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k)\n",
    "        alphas.append(alpha)\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha) * H_B + alpha * W_prev\n",
    "        MLambda = Lambda\n",
    "        X = B - MLambda\n",
    "\n",
    "        W = apply_A_inv_right(X, eigvals, Q, alpha)\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "        if k % 50 == 0 :\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.99\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method8(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=1000)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25001ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(0.5723, device='cuda:0', dtype=torch.float16) tensor(0.0677, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(0.8359, device='cuda:0', dtype=torch.float16) tensor(0.0694, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(0.6455, device='cuda:0', dtype=torch.float16) tensor(0.0790, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(0.5820, device='cuda:0', dtype=torch.float16) tensor(0.0732, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(0.6543, device='cuda:0', dtype=torch.float16) tensor(0.0704, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(0.6953, device='cuda:0', dtype=torch.float16) tensor(0.0700, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b295a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.3000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:02<00:22, 39.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5752, device='cuda:0', dtype=torch.float16) tensor(0.0674, device='cuda:0', dtype=torch.float16)\n",
      "Converged at iteration 100\n",
      "tensor(0.0947, device='cuda:0')\n",
      "time 2.5368621349334717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def delta_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end - alpha_start\n",
    "    else:\n",
    "        return alpha_schedule_exp(k, max_iter, alpha_start, alpha_end, gamma) - alpha_start\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "    A_start = torch.linalg.cholesky(A)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    \n",
    "    A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "    A2 = (-A1) @ (I - H_A) @ A_start\n",
    "\n",
    "    alpha_start = alpha\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha_start)\n",
    "        delta = alpha - alpha_start\n",
    "\n",
    "        if k % 20 == 0 and k <= 100:\n",
    "            alpha_start = alpha\n",
    "\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            \n",
    "            A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "            A2 = (-A1) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "\n",
    "        # A_inv = (1-alpha-delta) * H_A + (alpha + delta) * I\n",
    "        # A_inv = torch.linalg.cholesky(A_inv)\n",
    "        # A_inv = torch.cholesky_inverse(A_inv)\n",
    "\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2)\n",
    "        # W = B @ A_inv\n",
    "\n",
    "        # print(\"二阶近似误差\",torch.norm(((A_start + delta * A1 + delta**2 * A2) - A_inv)))\n",
    "\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=1000)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "131724cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.3000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 150/1000 [00:03<00:21, 40.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5771, device='cuda:0', dtype=torch.float16) tensor(0.0676, device='cuda:0', dtype=torch.float16)\n",
      "Converged at iteration 150\n",
      "tensor(0.0898, device='cuda:0')\n",
      "time 3.7241785526275635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def delta_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end - alpha_start\n",
    "    else:\n",
    "        return alpha_schedule_exp(k, max_iter, alpha_start, alpha_end, gamma) - alpha_start\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "    A_start = torch.linalg.cholesky(A)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    \n",
    "    A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "    A2 = (-A1) @ (I - H_A) @ A_start\n",
    "    A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "    alpha_start = alpha\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha_start)\n",
    "        delta = alpha - alpha_start\n",
    "\n",
    "        if k % 20 == 0 and k < 100:\n",
    "            alpha_start = alpha\n",
    "\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            \n",
    "            A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "            A2 = (-A1) @ (I - H_A) @ A_start\n",
    "            A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "\n",
    "        # A_inv = (1-alpha-delta) * H_A + (alpha + delta) * I\n",
    "        # A_inv = torch.linalg.cholesky(A_inv)\n",
    "        # A_inv = torch.cholesky_inverse(A_inv)\n",
    "\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2 + delta**3 * A3)\n",
    "        # W = B @ A_inv\n",
    "\n",
    "        # print(\"二阶近似误差\",torch.norm(((A_start + delta * A1 + delta**2 * A2) - A_inv)))\n",
    "\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=1000)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4849cfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.3000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [00:05<00:20, 39.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5771, device='cuda:0', dtype=torch.float16) tensor(0.0676, device='cuda:0', dtype=torch.float16)\n",
      "Converged at iteration 200\n",
      "tensor(0.0320, device='cuda:0')\n",
      "time 5.034789562225342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def delta_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end - alpha_start\n",
    "    else:\n",
    "        return alpha_schedule_exp(k, max_iter, alpha_start, alpha_end, gamma) - alpha_start\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "    A_start = torch.linalg.cholesky(A)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    \n",
    "    A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "    A2 = (-A1) @ (I - H_A) @ A_start\n",
    "    A3 = (-A2) @ (I - H_A) @ A_start\n",
    "    A4 = (-A3) @ (I - H_A) @ A_start\n",
    "\n",
    "    alpha_start = alpha\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha_start)\n",
    "        delta = alpha - alpha_start\n",
    "\n",
    "        if k % 20 == 0 and k < 100:\n",
    "            alpha_start = alpha\n",
    "\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            \n",
    "            A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "            A2 = (-A1) @ (I - H_A) @ A_start\n",
    "            A3 = (-A2) @ (I - H_A) @ A_start\n",
    "            A4 = (-A3) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "\n",
    "        # A_inv = (1-alpha-delta) * H_A + (alpha + delta) * I\n",
    "        # A_inv = torch.linalg.cholesky(A_inv)\n",
    "        # A_inv = torch.cholesky_inverse(A_inv)\n",
    "\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2 + delta**3 * A3 + delta**4 * A4)\n",
    "        # W = B @ A_inv\n",
    "\n",
    "        # print(\"二阶近似误差\",torch.norm(((A_start + delta * A1 + delta**2 * A2) - A_inv)))\n",
    "\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=1000)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c70f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(0.5752, device='cuda:0', dtype=torch.float16) tensor(0.0677, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(0.8398, device='cuda:0', dtype=torch.float16) tensor(0.0695, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(0.6489, device='cuda:0', dtype=torch.float16) tensor(0.0790, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(0.5791, device='cuda:0', dtype=torch.float16) tensor(0.0733, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(0.6582, device='cuda:0', dtype=torch.float16) tensor(0.0704, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(0.6973, device='cuda:0', dtype=torch.float16) tensor(0.0700, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482e8972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0678, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(0.8359, device='cuda:0', dtype=torch.float16) tensor(0.0695, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(0.6494, device='cuda:0', dtype=torch.float16) tensor(0.0791, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(0.5811, device='cuda:0', dtype=torch.float16) tensor(0.0733, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(0.6543, device='cuda:0', dtype=torch.float16) tensor(0.0705, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(0.6934, device='cuda:0', dtype=torch.float16) tensor(0.0701, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5b834df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50/1000 [00:01<00:31, 30.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 50\n",
      "tensor(0.0230, device='cuda:0')\n",
      "time 1.7006933689117432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    ratio = min(k / max_iter, 1.0)\n",
    "    return alpha_end - (alpha_end - alpha_start) * math.exp(-gamma * ratio)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=1000, lambda_zero=False, percdamp=.01):\n",
    "    # 数据类型和设备统一\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 转换为稀疏矩阵\n",
    "    M_sparse = M.to_sparse()\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    \n",
    "    # 预计算不变项\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    precomputed_term = I - H_A\n",
    "    \n",
    "    # 初始矩阵分解\n",
    "    A_start = torch.linalg.cholesky((1-alpha) * H_A + alpha * I)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    A1 = -(A_start @ precomputed_term @ A_start)\n",
    "    A2 = (-A1) @ precomputed_term @ A_start\n",
    "    \n",
    "    # 外推和自适应参数\n",
    "    alpha_threshold = 0.01\n",
    "    last_alpha = alpha\n",
    "    lambda_value = []\n",
    "    lambda_abs = []\n",
    "    out = []\n",
    "    \n",
    "    # Aitken 外推参数\n",
    "    aitken_freq = 10\n",
    "    aitken_history = []\n",
    "    \n",
    "    # 自适应步长参数\n",
    "    rho_min = 0.1\n",
    "    rho_max = 10.0\n",
    "    rho_adapt_factor = 1.1\n",
    "    \n",
    "    for k in tqdm(range(max_iter)):\n",
    "        # 更新 alpha\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha)\n",
    "        delta = alpha - last_alpha\n",
    "        \n",
    "        # 仅当 alpha 变化显著时重新计算分解\n",
    "        if abs(delta) > alpha_threshold:\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            A1 = -(A_start @ precomputed_term @ A_start)\n",
    "            A2 = (-A1) @ precomputed_term @ A_start\n",
    "            last_alpha = alpha\n",
    "        \n",
    "        W_prev = W.clone()\n",
    "        \n",
    "        # 主要迭代步骤\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "        \n",
    "        # 使用二阶近似计算 A_inv\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2)\n",
    "        \n",
    "        # 使用稀疏矩阵更新 Lambda\n",
    "        Lambda = Lambda + rho * (M_sparse.to_dense() * W)\n",
    "        \n",
    "        # 记录指标\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        \n",
    "        # # 记录 Aitken 历史\n",
    "        # aitken_history.append(W.clone())\n",
    "        # if len(aitken_history) > 3:\n",
    "        #     aitken_history.pop(0)\n",
    "        \n",
    "        # # 应用 Aitken 外推\n",
    "        # if k % aitken_freq == 0 and len(aitken_history) == 3:\n",
    "        #     x1, x2, x3 = aitken_history\n",
    "        #     diff1 = x3 - x2\n",
    "        #     diff2 = x2 - x1\n",
    "        #     numerator = torch.norm(diff1, p='fro')**2\n",
    "        #     denominator = torch.norm(diff1 - diff2, p='fro')\n",
    "        #     if denominator > 1e-10:\n",
    "        #         W_aitken = x3 - (numerator / denominator) * diff1\n",
    "        #         if torch.norm(W_aitken - W_prev) < torch.norm(W - W_prev):\n",
    "        #             W = W_aitken\n",
    "        #             aitken_history[-1] = W.clone()\n",
    "        \n",
    "        # # 自适应调整 rho\n",
    "        # if k > 0:\n",
    "        #     convergence_speed = torch.norm(W - W_prev) / (torch.norm(W_prev) + 1e-10)\n",
    "        #     if convergence_speed < 0.5:\n",
    "        #         rho = min(rho * rho_adapt_factor, rho_max)\n",
    "        #     elif convergence_speed > 0.8:\n",
    "        #         rho = max(rho / rho_adapt_factor, rho_min)\n",
    "        \n",
    "        # 收敛判断\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "    \n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "# 假设这些变量已经在别处定义\n",
    "# test_inps, test_outs, new_W\n",
    "\n",
    "# 计算 H_A 和 H_B\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "    \n",
    "    nsamples += 1\n",
    "    scale = math.sqrt(2 / nsamples)\n",
    "    x = scale * test_inps[i].cuda().to(torch.float32)\n",
    "    y = scale * test_outs[i][0].cuda().to(torch.float32)\n",
    "    \n",
    "    H_A += x @ x.T\n",
    "    H_B += y @ x.T\n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "\n",
    "# 参数设置\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "# 执行优化后的算法\n",
    "start = time.time()\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=1000)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5fc4e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(1.4824, device='cuda:0', dtype=torch.float16) tensor(0.1135, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(2.2832, device='cuda:0', dtype=torch.float16) tensor(0.1152, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(1.3125, device='cuda:0', dtype=torch.float16) tensor(0.1188, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(1.3926, device='cuda:0', dtype=torch.float16) tensor(0.1183, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(1.9961, device='cuda:0', dtype=torch.float16) tensor(0.1152, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(1.9434, device='cuda:0', dtype=torch.float16) tensor(0.1130, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb68680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing No Extrapolation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:02<00:21, 41.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 100\n",
      "tensor(0.0918, device='cuda:0')\n",
      "No Extrapolation took 2.42 seconds\n",
      "\n",
      "=== Testing Aitken Extrapolation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 46.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aitken Extrapolation took 21.56 seconds\n",
      "\n",
      "=== Testing Richardson Extrapolation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:04<00:38, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 100\n",
      "tensor(0.0346, device='cuda:0')\n",
      "Richardson Extrapolation took 4.29 seconds\n",
      "\n",
      "=== Testing Romberg Extrapolation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 23.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romberg Extrapolation took 43.21 seconds\n",
      "\n",
      "=== Testing Hybrid Extrapolation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 100/1000 [00:04<00:41, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 100\n",
      "tensor(0.0346, device='cuda:0')\n",
      "Hybrid Extrapolation took 4.63 seconds\n",
      "\n",
      "=== Results Comparison ===\n",
      "No Extrapolation: Iterations=101, Time=2.42s, Error=396.500000\n",
      "Aitken Extrapolation: Iterations=1000, Time=21.56s, Error=nan\n",
      "Richardson Extrapolation: Iterations=101, Time=4.29s, Error=396.250000\n",
      "Romberg Extrapolation: Iterations=1000, Time=43.21s, Error=nan\n",
      "Hybrid Extrapolation: Iterations=101, Time=4.63s, Error=396.250000\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
