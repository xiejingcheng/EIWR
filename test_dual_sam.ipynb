{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45bf723b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.3.1+cu121\n",
      "transformers 4.47.1\n",
      "accelerate 0.29.1\n",
      "# of gpus:  8\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from importlib.metadata import version\n",
    "\n",
    "from lib.prune import prune_wanda, prune_sparsegpt, prune_magnitude\n",
    "from lib.prune import prune_sparsegpt_ww, prune_wanda_ww\n",
    "from lib.eval import eval_ppl, eval_zero_shot\n",
    "from lib.esd_utils import get_esd_metrics\n",
    "\n",
    "from lib.sam_prune import prune_wanda_sam, prune_sparsegpt_sam\n",
    "\n",
    "from lib.utils import check_sparsity\n",
    "\n",
    "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map\n",
    "\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "from segment_anything.utils.dataset import dataset_dis, dataset_dis_val, dataset_duts, dataset_duts_te\n",
    "from segment_anything.utils.dataloader import get_im_gt_name_dict, create_dataloaders, RandomHFlip, Resize, LargeScaleJitter\n",
    "from segment_anything.utils import misc\n",
    "from segment_anything.utils.loss import norm_attn, pca_fit_transform, sig_ce_loss, dice_loss, mask_iou, sig_mae_score, f1_score\n",
    "\n",
    "\n",
    "print('torch', version('torch'))\n",
    "print('transformers', version('transformers'))\n",
    "print('accelerate', version('accelerate'))\n",
    "print('# of gpus: ', torch.cuda.device_count())\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "\n",
    "debug = False \n",
    "\n",
    "import time \n",
    "import heapq \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import transformers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from torch import nn\n",
    "\n",
    "# from .sam_sparsegpt import SparseGPT, SparseGPT_NoReconstruct, SparseGPTV3\n",
    "from lib.sam_layerwrapper import WrappedGPT, WrappedGPTV3, SparseGPTV3, SparseGPT, SparseGPTV2\n",
    "from lib.data import get_loaders, prepare_calibration_input_sam\n",
    "\n",
    "from lib.ablate import AblateGPT \n",
    "\n",
    "from lib.matmul_had import *\n",
    "from lib.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5722944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- create valid dataloader ---\n",
      "------------------------------ valid --------------------------------\n",
      "--->>> valid  dataset  0 / 1   DIS5K-VD <<<---\n",
      "-im- DIS5K-VD /h3cstore_ns/jcxie/SAM/SVD_SAM/data/data/DIS5K/DIS-VD/im :  470\n",
      "-gt- DIS5K-VD /h3cstore_ns/jcxie/SAM/SVD_SAM/data/data/DIS5K/DIS-VD/gt :  470\n",
      "1  valid dataloaders created\n",
      "image_encoder.pos_embed torch.Size([1, 64, 64, 768])\n",
      "image_encoder.patch_embed.proj.weight torch.Size([768, 3, 16, 16])\n",
      "image_encoder.patch_embed.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.0.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.0.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.0.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.0.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.0.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.0.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.0.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.0.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.0.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.0.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.0.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.0.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.0.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.0.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.1.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.1.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.1.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.1.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.1.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.1.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.1.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.1.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.1.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.1.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.1.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.1.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.1.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.1.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.2.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.2.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.2.attn.rel_pos_h torch.Size([127, 64])\n",
      "image_encoder.blocks.2.attn.rel_pos_w torch.Size([127, 64])\n",
      "image_encoder.blocks.2.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.2.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.2.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.2.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.2.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.2.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.2.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.2.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.2.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.2.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.3.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.3.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.3.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.3.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.3.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.3.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.3.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.3.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.3.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.3.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.3.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.3.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.3.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.3.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.4.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.4.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.4.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.4.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.4.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.4.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.4.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.4.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.4.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.4.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.4.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.4.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.4.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.4.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.5.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.5.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.5.attn.rel_pos_h torch.Size([127, 64])\n",
      "image_encoder.blocks.5.attn.rel_pos_w torch.Size([127, 64])\n",
      "image_encoder.blocks.5.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.5.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.5.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.5.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.5.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.5.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.5.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.5.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.5.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.5.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.6.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.6.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.6.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.6.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.6.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.6.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.6.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.6.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.6.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.6.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.6.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.6.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.6.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.6.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.7.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.7.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.7.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.7.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.7.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.7.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.7.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.7.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.7.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.7.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.7.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.7.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.7.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.7.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.8.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.8.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.8.attn.rel_pos_h torch.Size([127, 64])\n",
      "image_encoder.blocks.8.attn.rel_pos_w torch.Size([127, 64])\n",
      "image_encoder.blocks.8.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.8.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.8.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.8.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.8.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.8.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.8.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.8.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.8.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.8.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.9.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.9.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.9.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.9.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.9.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.9.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.9.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.9.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.9.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.9.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.9.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.9.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.9.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.9.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.10.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.10.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.10.attn.rel_pos_h torch.Size([27, 64])\n",
      "image_encoder.blocks.10.attn.rel_pos_w torch.Size([27, 64])\n",
      "image_encoder.blocks.10.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.10.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.10.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.10.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.10.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.10.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.10.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.10.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.10.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.10.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.blocks.11.norm1.weight torch.Size([768])\n",
      "image_encoder.blocks.11.norm1.bias torch.Size([768])\n",
      "image_encoder.blocks.11.attn.rel_pos_h torch.Size([127, 64])\n",
      "image_encoder.blocks.11.attn.rel_pos_w torch.Size([127, 64])\n",
      "image_encoder.blocks.11.attn.qkv.weight torch.Size([2304, 768])\n",
      "image_encoder.blocks.11.attn.qkv.bias torch.Size([2304])\n",
      "image_encoder.blocks.11.attn.proj.weight torch.Size([768, 768])\n",
      "image_encoder.blocks.11.attn.proj.bias torch.Size([768])\n",
      "image_encoder.blocks.11.norm2.weight torch.Size([768])\n",
      "image_encoder.blocks.11.norm2.bias torch.Size([768])\n",
      "image_encoder.blocks.11.mlp.lin1.weight torch.Size([3072, 768])\n",
      "image_encoder.blocks.11.mlp.lin1.bias torch.Size([3072])\n",
      "image_encoder.blocks.11.mlp.lin2.weight torch.Size([768, 3072])\n",
      "image_encoder.blocks.11.mlp.lin2.bias torch.Size([768])\n",
      "image_encoder.neck.0.weight torch.Size([256, 768, 1, 1])\n",
      "image_encoder.neck.1.weight torch.Size([256])\n",
      "image_encoder.neck.1.bias torch.Size([256])\n",
      "image_encoder.neck.2.weight torch.Size([256, 256, 3, 3])\n",
      "image_encoder.neck.3.weight torch.Size([256])\n",
      "image_encoder.neck.3.bias torch.Size([256])\n",
      "prompt_encoder.point_embeddings.0.weight torch.Size([1, 256])\n",
      "prompt_encoder.point_embeddings.1.weight torch.Size([1, 256])\n",
      "prompt_encoder.point_embeddings.2.weight torch.Size([1, 256])\n",
      "prompt_encoder.point_embeddings.3.weight torch.Size([1, 256])\n",
      "prompt_encoder.not_a_point_embed.weight torch.Size([1, 256])\n",
      "prompt_encoder.mask_downscaling.0.weight torch.Size([4, 1, 2, 2])\n",
      "prompt_encoder.mask_downscaling.0.bias torch.Size([4])\n",
      "prompt_encoder.mask_downscaling.1.weight torch.Size([4])\n",
      "prompt_encoder.mask_downscaling.1.bias torch.Size([4])\n",
      "prompt_encoder.mask_downscaling.3.weight torch.Size([16, 4, 2, 2])\n",
      "prompt_encoder.mask_downscaling.3.bias torch.Size([16])\n",
      "prompt_encoder.mask_downscaling.4.weight torch.Size([16])\n",
      "prompt_encoder.mask_downscaling.4.bias torch.Size([16])\n",
      "prompt_encoder.mask_downscaling.6.weight torch.Size([256, 16, 1, 1])\n",
      "prompt_encoder.mask_downscaling.6.bias torch.Size([256])\n",
      "prompt_encoder.no_mask_embed.weight torch.Size([1, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm1.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm1.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm2.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm2.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.mlp.lin1.weight torch.Size([2048, 256])\n",
      "mask_decoder.transformer.layers.0.mlp.lin1.bias torch.Size([2048])\n",
      "mask_decoder.transformer.layers.0.mlp.lin2.weight torch.Size([256, 2048])\n",
      "mask_decoder.transformer.layers.0.mlp.lin2.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm3.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm3.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm4.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.norm4.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj.weight torch.Size([256, 256])\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm1.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm1.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm2.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm2.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.mlp.lin1.weight torch.Size([2048, 256])\n",
      "mask_decoder.transformer.layers.1.mlp.lin1.bias torch.Size([2048])\n",
      "mask_decoder.transformer.layers.1.mlp.lin2.weight torch.Size([256, 2048])\n",
      "mask_decoder.transformer.layers.1.mlp.lin2.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm3.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm3.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm4.weight torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.norm4.bias torch.Size([256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight torch.Size([256, 128])\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj.weight torch.Size([128, 256])\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj.bias torch.Size([128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj.weight torch.Size([256, 128])\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj.bias torch.Size([256])\n",
      "mask_decoder.transformer.norm_final_attn.weight torch.Size([256])\n",
      "mask_decoder.transformer.norm_final_attn.bias torch.Size([256])\n",
      "mask_decoder.iou_token.weight torch.Size([1, 256])\n",
      "mask_decoder.mask_tokens.weight torch.Size([4, 256])\n",
      "mask_decoder.output_upscaling.0.weight torch.Size([256, 64, 2, 2])\n",
      "mask_decoder.output_upscaling.0.bias torch.Size([64])\n",
      "mask_decoder.output_upscaling.1.weight torch.Size([64])\n",
      "mask_decoder.output_upscaling.1.bias torch.Size([64])\n",
      "mask_decoder.output_upscaling.3.weight torch.Size([64, 32, 2, 2])\n",
      "mask_decoder.output_upscaling.3.bias torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2.weight torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2.bias torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2.weight torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2.bias torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2.weight torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2.bias torch.Size([32])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.1.weight torch.Size([256, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.1.bias torch.Size([256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.2.weight torch.Size([32, 256])\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.2.bias torch.Size([32])\n",
      "mask_decoder.iou_prediction_head.layers.0.weight torch.Size([256, 256])\n",
      "mask_decoder.iou_prediction_head.layers.0.bias torch.Size([256])\n",
      "mask_decoder.iou_prediction_head.layers.1.weight torch.Size([256, 256])\n",
      "mask_decoder.iou_prediction_head.layers.1.bias torch.Size([256])\n",
      "mask_decoder.iou_prediction_head.layers.2.weight torch.Size([4, 256])\n",
      "mask_decoder.iou_prediction_head.layers.2.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "class PruneConfig:\n",
    "    def __init__(self):\n",
    "        self.model = '/h3cstore_ns/jcxie/SAM/SVD_SAM/pretrain/sam_vit_b_01ec64.pth'\n",
    "        self.model_name = 'vit_b'\n",
    "        self.seed = 0\n",
    "        self.nsamples = 128\n",
    "        self.sparsity_ratio = 0.5\n",
    "        self.sparsity_type = \"unstructured\"\n",
    "        self.prune_method = \"sparsegpt_silu_ww\"\n",
    "        self.cache_dir = \"llm_weights\"\n",
    "        self.use_variant = False\n",
    "        self.save = '/h3cstore_ns/jcxie/LISA/wanda-main/ckpt'\n",
    "        self.save_model = None\n",
    "        self.exclude = 'gate_proj'\n",
    "        self.ww_metric = \"alpha_peak\"\n",
    "        self.ww_metric_cache = \"/h3cstore_ns/jcxie/LISA/wanda-main/data/llama2-7b-hf\"\n",
    "        self.epsilon = 0.3\n",
    "        self.mapping_type = \"block_wise\"\n",
    "        self.Hyper_m = 3.0\n",
    "        self.Lamda = 0.20\n",
    "        self.eval_zero_shot = False\n",
    "args = PruneConfig()\n",
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "\n",
    "if \"ww\" in args.prune_method and not os.path.exists(f\"{args.ww_metric_cache}/{args.ww_metric}.npy\"):\n",
    "    metric_values = get_esd_metrics(args.model, args.ww_metric, args.cache_dir)\n",
    "    np.save(f\"{args.ww_metric_cache}/{args.ww_metric}.npy\", metric_values)\n",
    "\n",
    "# Handling n:m sparsity\n",
    "prune_n, prune_m = 0, 0\n",
    "if args.sparsity_type != \"unstructured\":\n",
    "    assert args.sparsity_ratio == 0.5, \"sparsity ratio must be 0.5 for structured N:M sparsity\"\n",
    "    prune_n, prune_m = map(int, args.sparsity_type.split(\":\"))\n",
    "\n",
    "model = sam_model_registry[args.model_name](args.model).cuda()\n",
    "\n",
    "valid_datasets = [dataset_dis_val]\n",
    "\n",
    "input_size = [1024,1024]\n",
    "print(\"--- create valid dataloader ---\")\n",
    "valid_im_gt_list = get_im_gt_name_dict(valid_datasets, flag=\"valid\")\n",
    "valid_dataloaders, valid_datasets = create_dataloaders(valid_im_gt_list,\n",
    "                                                        my_transforms = [\n",
    "                                                                    Resize(input_size)\n",
    "                                                                ],\n",
    "                                                        batch_size=1,\n",
    "                                                        training=False)\n",
    "print(len(valid_dataloaders), \" valid dataloaders created\")\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba306908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 attn.qkv\n",
      "0 attn.proj\n",
      "0 mlp.lin1\n",
      "0 mlp.lin2\n",
      "1 attn.qkv\n",
      "1 attn.proj\n",
      "1 mlp.lin1\n",
      "1 mlp.lin2\n",
      "2 attn.qkv\n",
      "2 attn.proj\n",
      "2 mlp.lin1\n",
      "2 mlp.lin2\n",
      "3 attn.qkv\n",
      "3 attn.proj\n",
      "3 mlp.lin1\n",
      "3 mlp.lin2\n",
      "4 attn.qkv\n",
      "4 attn.proj\n",
      "4 mlp.lin1\n",
      "4 mlp.lin2\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "dataloader = valid_dataloaders\n",
    "ratios = None\n",
    "dual_ascent = False\n",
    "valid_out = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    dev = device\n",
    "\n",
    "    inps, outs = prepare_calibration_input_sam(args, model, dataloader[0], args.nsamples)\n",
    "\n",
    "    layers = model.image_encoder.blocks\n",
    "    layer_num = len(find_layers(layers))\n",
    "\n",
    "    if ratios is None:\n",
    "        ratios = [args.sparsity_ratio for i in range(layer_num)]\n",
    "    k=0\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "            if i == 4 and name == 'mlp.lin1':\n",
    "                gpts[name] = SparseGPTV3(subset[name])\n",
    "            else:\n",
    "                gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad(): \n",
    "                outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        if i == 4:\n",
    "            test_H = gpts['mlp.lin1'].H\n",
    "            test_H_B = gpts['mlp.lin1'].H_B\n",
    "            ora_W = model.image_encoder.blocks[i].mlp.lin1.weight.data\n",
    "\n",
    "        for name in subset:\n",
    "            print(i, name)\n",
    "            gpts[name].fasterprune(ratios[k], prune_n=prune_n, prune_m=prune_m, percdamp=0.01, blocksize=128)\n",
    "\n",
    "            if dual_ascent:\n",
    "                flag, alpha, beta = gpts[name].get_args()\n",
    "                min_iter = 0\n",
    "                if flag:\n",
    "                    gpts[name].dual_ascent2(beta = beta, alpha = alpha, min_iter=min_iter, theld= args.dual_theld)\n",
    "                    gpts[name].del_valid()\n",
    "                else:\n",
    "                    del gpts[name].H, gpts[name].H_B\n",
    "\n",
    "            gpts[name].free()\n",
    "            k+=1\n",
    "\n",
    "        if i == 4:\n",
    "            new_W = gpts['mlp.lin1'].layer.weight.data\n",
    "            break\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "\n",
    "        layers[i] = layer\n",
    "        inps, outs = outs, inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7afacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 attn.qkv\n",
      "0 attn.proj\n",
      "0 mlp.lin1\n",
      "0 mlp.lin2\n",
      "1 attn.qkv\n",
      "1 attn.proj\n",
      "1 mlp.lin1\n",
      "1 mlp.lin2\n",
      "2 attn.qkv\n",
      "2 attn.proj\n",
      "2 mlp.lin1\n",
      "2 mlp.lin2\n",
      "3 attn.qkv\n",
      "3 attn.proj\n",
      "3 mlp.lin1\n",
      "3 mlp.lin2\n",
      "4 attn.qkv\n",
      "4 attn.proj\n",
      "4 mlp.lin1\n",
      "4 mlp.lin2\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "dataloader = valid_dataloaders\n",
    "ratios = None\n",
    "dual_ascent = False\n",
    "valid_out = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    dev = device\n",
    "\n",
    "    inps, outs = prepare_calibration_input_sam(args, model, dataloader[0], args.nsamples)\n",
    "\n",
    "    layers = model.image_encoder.blocks\n",
    "    layer_num = len(find_layers(layers))\n",
    "\n",
    "    if ratios is None:\n",
    "        ratios = [args.sparsity_ratio for i in range(layer_num)]\n",
    "    k=0\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "            if i == 4 and name == 'mlp.lin1':\n",
    "                gpts[name] = SparseGPTV2(subset[name])\n",
    "            else:\n",
    "                gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad(): \n",
    "                outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        if i == 4:\n",
    "            test_inps = gpts['mlp.lin1'].inps\n",
    "            test_outs = gpts['mlp.lin1'].outs\n",
    "            ora_W = model.image_encoder.blocks[i].mlp.lin1.weight.data\n",
    "\n",
    "        for name in subset:\n",
    "            print(i, name)\n",
    "            gpts[name].fasterprune(ratios[k], prune_n=prune_n, prune_m=prune_m, percdamp=0.01, blocksize=128)\n",
    "\n",
    "            if dual_ascent:\n",
    "                flag, alpha, beta = gpts[name].get_args()\n",
    "                min_iter = 0\n",
    "                if flag:\n",
    "                    gpts[name].dual_ascent2(beta = beta, alpha = alpha, min_iter=min_iter, theld= args.dual_theld)\n",
    "                    gpts[name].del_valid()\n",
    "                else:\n",
    "                    del gpts[name].H, gpts[name].H_B\n",
    "\n",
    "            gpts[name].free()\n",
    "            k+=1\n",
    "\n",
    "        if i == 4:\n",
    "            new_W = gpts['mlp.lin1'].layer.weight.data\n",
    "            break\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(inps[j].unsqueeze(0))[0]\n",
    "\n",
    "        layers[i] = layer\n",
    "        inps, outs = outs, inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8849c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ora_W torch.Size([3072, 768])\n",
      "new_W torch.Size([3072, 768])\n",
      "test_inps torch.Size([128, 768, 4096])\n",
      "test_outs torch.Size([128, 3072, 4096])\n"
     ]
    }
   ],
   "source": [
    "test_inps = torch.stack(test_inps, dim=0)\n",
    "test_outs = torch.stack(test_outs, dim=0)\n",
    "\n",
    "print('ora_W', ora_W.shape)\n",
    "print('new_W', new_W.shape)\n",
    "print('test_inps', test_inps.shape)\n",
    "print('test_outs', test_outs.shape)\n",
    "import torch\n",
    "ora_W = ora_W.cuda()\n",
    "new_W = new_W.cuda()\n",
    "test_inps = test_inps.cpu()\n",
    "test_outs = test_outs.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a58186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000, device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(new_W == 0).sum() / new_W.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7cf6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.5000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 180/10000 [00:00<00:20, 490.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1055, device='cuda:0') tensor(0.8515, device='cuda:0')\n",
      "Converged at iteration 180\n",
      "tensor(0.0298, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "def dual_ascent_method3(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=3e-2, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    else:\n",
    "        term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "        term2 = alpha * (W - W_old)\n",
    "        Lambda = -M * (term1 + term2)\n",
    "\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        # 保存上一次的 W\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        # 更新 W\n",
    "        A = (beta + gama) * H_A + alpha * torch.eye(H_A.shape[0], device=H_A.device)\n",
    "        try:\n",
    "            damp = percdamp * torch.mean(torch.diag(A))\n",
    "            diag = torch.arange(A.shape[-1], device=A.device)\n",
    "            A[diag, diag] += damp\n",
    "            A = torch.linalg.cholesky(A)\n",
    "            A_inv = torch.cholesky_inverse(A)\n",
    "            # A_inv = torch.linalg.cholesky(A, upper=True)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Cholesky decomposition failed: {e}. Falling back to direct inverse.\")\n",
    "            raise e\n",
    "        \n",
    "        B = beta * H_B + alpha * W_old\n",
    "        W = torch.mm(B - (M * Lambda), A_inv)\n",
    "\n",
    "        # 更新 Lambda\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "\n",
    "        if k % 10 == 0 :\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float32) @ test_inps[0].cuda() - test_outs[0].cuda()).abs().max(), (W.to(torch.float32) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float32) @ test_inps[0].cuda() - test_outs[0].cuda()).abs().max(), (W.to(torch.float32) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float32)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "# H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    # H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    # H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "H_B = ora_W @ H_A\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.99\n",
    "alpha = 0.01\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "W_update = dual_ascent_method3(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70d74950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n",
      "tensor(7.3101) tensor(0.9603)\n",
      "0 tensor(0.5665, device='cuda:0') tensor(0.0662, device='cuda:0')\n",
      "0 tensor(0.5296, device='cuda:0') tensor(0.0501, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.0926) tensor(0.9997)\n",
      "32 tensor(0.6468, device='cuda:0') tensor(0.0814, device='cuda:0')\n",
      "32 tensor(0.5481, device='cuda:0') tensor(0.0730, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(6.8198) tensor(0.9867)\n",
      "64 tensor(0.5995, device='cuda:0') tensor(0.0764, device='cuda:0')\n",
      "64 tensor(0.5153, device='cuda:0') tensor(0.0665, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.1617) tensor(0.9814)\n",
      "125 tensor(0.7372, device='cuda:0') tensor(0.0766, device='cuda:0')\n",
      "125 tensor(0.5629, device='cuda:0') tensor(0.0660, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.2366) tensor(0.9635)\n",
      "126 tensor(0.6166, device='cuda:0') tensor(0.0717, device='cuda:0')\n",
      "126 tensor(0.4724, device='cuda:0') tensor(0.0597, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "\n",
    "for i in [0, 32, 64, 125, 126]:\n",
    "    W_update = W_update - M.to(torch.float32) * W_update\n",
    "    print(test_outs[i].abs().max(), test_outs[i].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - ora_W @ test_inps[i].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - ora_W @ test_inps[i].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - ora_W @ test_inps[i].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - ora_W @ test_inps[i].cuda()).abs().mean())\n",
    "    print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41c3071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n",
      "tensor(7.3101) tensor(0.9603)\n",
      "0 tensor(3.2922, device='cuda:0') tensor(0.8560, device='cuda:0')\n",
      "0 tensor(7.0352, device='cuda:0') tensor(0.8510, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.0926) tensor(0.9997)\n",
      "32 tensor(3.3288, device='cuda:0') tensor(0.8556, device='cuda:0')\n",
      "32 tensor(7.5410, device='cuda:0') tensor(0.9369, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(6.8198) tensor(0.9867)\n",
      "64 tensor(3.2881, device='cuda:0') tensor(0.8565, device='cuda:0')\n",
      "64 tensor(7.6727, device='cuda:0') tensor(0.9397, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.1617) tensor(0.9814)\n",
      "125 tensor(3.3770, device='cuda:0') tensor(0.8556, device='cuda:0')\n",
      "125 tensor(7.6420, device='cuda:0') tensor(0.8889, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.2366) tensor(0.9635)\n",
      "126 tensor(3.2769, device='cuda:0') tensor(0.8548, device='cuda:0')\n",
      "126 tensor(7.0017, device='cuda:0') tensor(0.8600, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "\n",
    "for i in [0, 32, 64, 125, 126]:\n",
    "    W_update = W_update - M.to(torch.float32) * W_update\n",
    "    print(test_outs[i].abs().max(), test_outs[i].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ec2f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8000, device='cuda:0')\n",
      "tensor(7.3737) tensor(0.9544)\n",
      "0 tensor(6.7985, device='cuda:0') tensor(0.8591, device='cuda:0')\n",
      "0 tensor(6.7128, device='cuda:0') tensor(0.8431, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.5892) tensor(1.0019)\n",
      "32 tensor(7.2514, device='cuda:0') tensor(0.8324, device='cuda:0')\n",
      "32 tensor(7.3916, device='cuda:0') tensor(0.9059, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.4044) tensor(0.9736)\n",
      "64 tensor(6.4009, device='cuda:0') tensor(0.7625, device='cuda:0')\n",
      "64 tensor(8.5443, device='cuda:0') tensor(0.9947, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.8228) tensor(0.9756)\n",
      "125 tensor(7.3288, device='cuda:0') tensor(0.8225, device='cuda:0')\n",
      "125 tensor(8.1914, device='cuda:0') tensor(0.8897, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.4734) tensor(0.9543)\n",
      "126 tensor(7.4414, device='cuda:0') tensor(0.8072, device='cuda:0')\n",
      "126 tensor(7.4313, device='cuda:0') tensor(0.8500, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 64, 125, 126]:\n",
    "    W_update = W_update - M.to(torch.float32) * W_update\n",
    "    print(test_outs[i].abs().max(), test_outs[i].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1987a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.5000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 800/10000 [00:03<00:40, 224.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration 800\n",
      "tensor(0.0096, device='cuda:0')\n",
      "tensor(5.2047, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# test_x = test_inps[0]\n",
    "# test_y = test_outs[0]\n",
    "\n",
    "H_A = test_H.clone().to(\"cuda\")\n",
    "H_B = test_H_B.clone().to(\"cuda\")\n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "\n",
    "def dual_ascent2(W_old, H_A, H_B, beta=0.01, alpha=0.99, gama=0.0000, rho=1, epsilon=1e-2, max_iter=1000, lambda_zero=False, percdamp=.01, min_iter=300, theld=0.07):\n",
    "    \n",
    "\n",
    "    old_score = 0\n",
    "    new_score = 0\n",
    "    # for v in range(len(self.valid_inps)):\n",
    "    #     old_score += (W_old @ self.valid_inps[v] - self.valid_outs[v]).abs().mean()\n",
    "    # old_score = old_score / len(self.valid_inps)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    M = (W_old == 0).to(torch.float32)\n",
    "\n",
    "\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    else:\n",
    "        term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "        term2 = alpha * (W - W_old)\n",
    "        Lambda = -M * (term1 + term2)\n",
    "\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        # 保存上一次的 W\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        # 更新 W\n",
    "        A = (beta + gama) * H_A + alpha * torch.eye(H_A.shape[0], device=H_A.device)\n",
    "        try:\n",
    "            damp = percdamp * torch.mean(torch.diag(A))\n",
    "            diag = torch.arange(A.shape[-1], device=A.device)\n",
    "            A[diag, diag] += damp\n",
    "            A = torch.linalg.cholesky(A)\n",
    "            A_inv = torch.cholesky_inverse(A)\n",
    "            # A_inv = torch.linalg.cholesky(A, upper=True)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Cholesky decomposition failed: {e}. Falling back to direct inverse.\")\n",
    "            raise e\n",
    "        \n",
    "        B = beta * H_B + alpha * W_old\n",
    "        W = torch.mm(B - (M * Lambda), A_inv)\n",
    "\n",
    "        # 更新 Lambda\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        # 收敛判断\n",
    "        if k % 100 == 0 :\n",
    "            # W = W - M * W\n",
    "            \n",
    "            if k > min_iter:\n",
    "                if torch.norm(W - W_prev) < epsilon:\n",
    "                    print(f\"Converged at iteration {k}\")\n",
    "                    print(torch.norm(W - W_prev))\n",
    "                    break\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    W = W - M * W\n",
    "    print(torch.norm(W - W_prev))\n",
    "\n",
    "    # for v in range(len(self.valid_inps)):\n",
    "    #     new_score = (W.to(torch.float32) @ self.valid_inps[v] - self.valid_outs[v]).abs().mean()\n",
    "    # new_score = new_score / len(self.valid_inps)\n",
    "\n",
    "    # print(\"old_score:\", old_score, \"new_score:\", new_score)\n",
    "    # if new_score < (old_score * (1 - theld)):\n",
    "    #     print(\"Converged!\")\n",
    "    #     self.layer.weight.data = W.to(torch.float32)\n",
    "    # else:\n",
    "    #     print(\"Not converged!\")\n",
    "    #     self.layer.weight.data = W_old.to(torch.float32)\n",
    "\n",
    "    # print(\"Dual ascent finished!\")\n",
    "    del W_old, H_A, H_B, A, B, Lambda\n",
    "    torch.cuda.empty_cache()\n",
    "    return W.to(torch.float32)\n",
    "\n",
    "W_update = dual_ascent2(W_old, H_A, H_B, beta=0.99, alpha=0.01, gama=0.0000, rho=1, lambda_zero=True, max_iter=10000, percdamp=.01, min_iter=300, theld=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f3e7a9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_inps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_inps\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_outs\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_inps' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_inps.shape)\n",
    "print(test_outs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59bef35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n",
      "tensor(7.3101) tensor(0.9603)\n",
      "tensor(6.9375, device='cuda:0') tensor(0.8099, device='cuda:0')\n",
      "0 tensor(2.7277, device='cuda:0') tensor(0.1954, device='cuda:0')\n",
      "0 tensor(3.2381, device='cuda:0') tensor(0.8544, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.0926) tensor(0.9997)\n",
      "tensor(6.9747, device='cuda:0') tensor(0.8353, device='cuda:0')\n",
      "32 tensor(2.6844, device='cuda:0') tensor(0.2219, device='cuda:0')\n",
      "32 tensor(3.2661, device='cuda:0') tensor(0.8551, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(6.8198) tensor(0.9867)\n",
      "tensor(6.5462, device='cuda:0') tensor(0.8258, device='cuda:0')\n",
      "64 tensor(2.7837, device='cuda:0') tensor(0.2123, device='cuda:0')\n",
      "64 tensor(3.2789, device='cuda:0') tensor(0.8557, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.1617) tensor(0.9814)\n",
      "tensor(6.7156, device='cuda:0') tensor(0.8353, device='cuda:0')\n",
      "125 tensor(2.6627, device='cuda:0') tensor(0.1968, device='cuda:0')\n",
      "125 tensor(3.2791, device='cuda:0') tensor(0.8551, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n",
      "tensor(7.2366) tensor(0.9635)\n",
      "tensor(6.9681, device='cuda:0') tensor(0.8145, device='cuda:0')\n",
      "126 tensor(2.6409, device='cuda:0') tensor(0.1971, device='cuda:0')\n",
      "126 tensor(3.2532, device='cuda:0') tensor(0.8544, device='cuda:0')\n",
      "-----------\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 64, 125, 126]:\n",
    "    W_update = W_update - M.to(torch.float32) * W_update\n",
    "    print(test_outs[i].abs().max(), test_outs[i].abs().mean())\n",
    "    print((W_update @ test_inps[i].cuda()).abs().max(), (W_update @ test_inps[i].cuda()).abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i].cuda()).abs().mean())\n",
    "    print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
