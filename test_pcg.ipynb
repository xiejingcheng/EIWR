{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "ora_W = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_ora_W.npy')\n",
    "new_W = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_new_W.npy')\n",
    "test_inps = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_test_inps.npy')\n",
    "test_outs = np.load('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/9_self_attn.v_proj_test_outs.npy')\n",
    "\n",
    "ora_W = torch.tensor(ora_W).to(torch.float16).cuda()\n",
    "new_W = torch.tensor(new_W).to(torch.float16).cuda()\n",
    "test_inps = torch.tensor(test_inps).to(torch.float16)\n",
    "test_outs = torch.tensor(test_outs).to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9479c902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:25<00:00, 39.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "XTX = H_A\n",
    "W_hat = new_W.to(torch.float32).cuda()\n",
    "S = (W_hat != 0).to(torch.float32).cuda()\n",
    "W_0 = ora_W.to(torch.float32).cuda()\n",
    "\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def vectorized_pcg_solver(XTX, W_0, S, max_iter=100, tol=1e-6, device='cuda'):\n",
    "\n",
    "    XTX = XTX.to(device)\n",
    "    W_0 = W_0.to(device)\n",
    "    S = S.to(device)\n",
    "    \n",
    "\n",
    "    R0 = XTX @ W_0\n",
    "    \n",
    "\n",
    "    R0 = R0 * S\n",
    "    \n",
    "    # 计算预条件子 M = Diag(XTX) 的逆\n",
    "    M_diag = torch.diag(XTX)\n",
    "    # 避免除以零，添加小常数\n",
    "    M_inv_diag = 1.0 / (M_diag + 1e-10)\n",
    "    M_inv = torch.diag(M_inv_diag)\n",
    "    \n",
    "    # 计算 Z0 = M^(-1)R0\n",
    "    Z0 = M_inv @ R0\n",
    "    \n",
    "    # 初始化P0 = Z0\n",
    "    P0 = Z0.clone()\n",
    "    \n",
    "    # 初始化迭代变量\n",
    "    R = R0\n",
    "    Z = Z0\n",
    "    P = P0\n",
    "    W = torch.zeros_like(W_0)  # 初始解设为零矩阵\n",
    "    \n",
    "    # 记录收敛历史\n",
    "    residuals = []\n",
    "    \n",
    "    # 开始迭代\n",
    "    for t in tqdm(range(max_iter)):\n",
    "        # 计算 X⊤XPt\n",
    "        XTX_P = XTX @ P\n",
    "        \n",
    "        # 计算 αt = Tr(Rt⊤Zt)/ Tr(Pt⊤X⊤XPt)\n",
    "        # 注意：在矩阵形式下，Tr(A⊤B) 等价于 A 和 B 的逐元素乘积之和\n",
    "        numerator = torch.sum(R * Z)\n",
    "        denominator = torch.sum(P * XTX_P)\n",
    "        \n",
    "        # 避免除以零\n",
    "        if denominator == 0:\n",
    "            break\n",
    "            \n",
    "        alpha = numerator / denominator\n",
    "        \n",
    "        # 更新解 Wt+1 = Wt + αtPt\n",
    "        W = W + alpha * P\n",
    "        \n",
    "        # 更新残差 Rt+1 = Rt - αtX⊤XPt\n",
    "        R_next = R - alpha * XTX_P\n",
    "        \n",
    "        # 投影到支撑集S\n",
    "        R_next = R_next * S\n",
    "        \n",
    "        # 计算残差范数，用于收敛检查\n",
    "        residual_norm = torch.norm(R_next)\n",
    "        residuals.append(residual_norm.item())\n",
    "        \n",
    "        # 检查收敛\n",
    "        if residual_norm < tol:\n",
    "            print(f\"Converged at iteration {t+1} with residual norm {residual_norm:.6e}\")\n",
    "            break\n",
    "            \n",
    "        # 计算 Zt+1 = M^(-1)Rt+1\n",
    "        Z_next = M_inv @ R_next\n",
    "        \n",
    "        # 计算 βt = Tr(Rt+1⊤Zt+1)/ Tr(Rt⊤Zt)\n",
    "        beta = torch.sum(R_next * Z_next) / (torch.sum(R * Z) + 1e-10)\n",
    "        \n",
    "        # 更新搜索方向 Pt+1 = Zt+1 + βtPt\n",
    "        P = Z_next + beta * P\n",
    "        \n",
    "        # 更新迭代变量\n",
    "        R = R_next\n",
    "        Z = Z_next\n",
    "    \n",
    "    return W, residuals\n",
    "\n",
    "W_final, residuals = vectorized_pcg_solver(XTX, W_0, S, max_iter=1000, tol=1e-6, device='cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f3f41b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 运行论文中的PCG算法 =====\n",
      "初始化残差范数: 5.114889e+05，收敛阈值: 5.114889e+01\n",
      " 迭代 |       残差范数 |       相对误差\n",
      "001 | 9.219654e+04 | 1.802513e-01\n",
      "002 | 1.896738e+05 | 3.708268e-01\n",
      "003 | 1.641573e+05 | 3.209402e-01\n",
      "004 | 7.987156e+04 | 1.561550e-01\n",
      "005 | 1.016196e+05 | 1.986741e-01\n",
      "006 | 1.121187e+05 | 2.192006e-01\n",
      "007 | 8.343537e+04 | 1.631225e-01\n",
      "008 | 5.531611e+04 | 1.081472e-01\n",
      "009 | 6.789459e+04 | 1.327391e-01\n",
      "010 | 6.695902e+04 | 1.309100e-01\n",
      "011 | 4.935154e+04 | 9.648602e-02\n",
      "012 | 4.619877e+04 | 9.032213e-02\n",
      "013 | 4.494494e+04 | 8.787079e-02\n",
      "014 | 3.975726e+04 | 7.772849e-02\n",
      "015 | 3.642823e+04 | 7.121997e-02\n",
      "016 | 3.335642e+04 | 6.521436e-02\n",
      "017 | 3.191435e+04 | 6.239499e-02\n",
      "018 | 3.102917e+04 | 6.066440e-02\n",
      "019 | 2.776988e+04 | 5.429223e-02\n",
      "020 | 2.376853e+04 | 4.646929e-02\n",
      "021 | 2.267966e+04 | 4.434047e-02\n",
      "022 | 2.502985e+04 | 4.893528e-02\n",
      "023 | 2.210369e+04 | 4.321440e-02\n",
      "024 | 1.811390e+04 | 3.541405e-02\n",
      "025 | 2.247052e+04 | 4.393158e-02\n",
      "026 | 2.003433e+04 | 3.916866e-02\n",
      "027 | 1.682755e+04 | 3.289915e-02\n",
      "028 | 1.501483e+04 | 2.935513e-02\n",
      "029 | 1.524458e+04 | 2.980432e-02\n",
      "030 | 1.455664e+04 | 2.845934e-02\n",
      "031 | 1.348855e+04 | 2.637114e-02\n",
      "032 | 1.336808e+04 | 2.613561e-02\n",
      "033 | 1.476663e+04 | 2.886989e-02\n",
      "034 | 1.291877e+04 | 2.525719e-02\n",
      "035 | 1.144050e+04 | 2.236705e-02\n",
      "036 | 1.108752e+04 | 2.167695e-02\n",
      "037 | 1.035296e+04 | 2.024084e-02\n",
      "038 | 9.910810e+03 | 1.937639e-02\n",
      "039 | 1.009642e+04 | 1.973928e-02\n",
      "040 | 9.777785e+03 | 1.911632e-02\n",
      "041 | 8.895227e+03 | 1.739085e-02\n",
      "042 | 8.504575e+03 | 1.662710e-02\n",
      "043 | 8.475151e+03 | 1.656957e-02\n",
      "044 | 8.064232e+03 | 1.576619e-02\n",
      "045 | 8.104026e+03 | 1.584399e-02\n",
      "046 | 8.414367e+03 | 1.645073e-02\n",
      "047 | 8.543320e+03 | 1.670284e-02\n",
      "048 | 7.626857e+03 | 1.491109e-02\n",
      "049 | 7.333893e+03 | 1.433832e-02\n",
      "050 | 7.033404e+03 | 1.375084e-02\n",
      "051 | 7.280088e+03 | 1.423313e-02\n",
      "052 | 7.522411e+03 | 1.470689e-02\n",
      "053 | 6.967702e+03 | 1.362239e-02\n",
      "054 | 6.712110e+03 | 1.312269e-02\n",
      "055 | 6.758411e+03 | 1.321321e-02\n",
      "056 | 6.880461e+03 | 1.345183e-02\n",
      "057 | 7.437725e+03 | 1.454132e-02\n",
      "058 | 6.546193e+03 | 1.279831e-02\n",
      "059 | 6.013398e+03 | 1.175665e-02\n",
      "060 | 6.155408e+03 | 1.203429e-02\n",
      "061 | 6.244181e+03 | 1.220785e-02\n",
      "062 | 5.260919e+03 | 1.028550e-02\n",
      "063 | 5.479367e+03 | 1.071258e-02\n",
      "064 | 6.010758e+03 | 1.175149e-02\n",
      "065 | 5.250864e+03 | 1.026584e-02\n",
      "066 | 6.045611e+03 | 1.181963e-02\n",
      "067 | 5.625467e+03 | 1.099822e-02\n",
      "068 | 5.040412e+03 | 9.854390e-03\n",
      "069 | 4.478208e+03 | 8.755239e-03\n",
      "070 | 4.866642e+03 | 9.514658e-03\n",
      "071 | 5.013651e+03 | 9.802071e-03\n",
      "072 | 4.367896e+03 | 8.539570e-03\n",
      "073 | 4.205528e+03 | 8.222129e-03\n",
      "074 | 4.695260e+03 | 9.179593e-03\n",
      "075 | 4.592236e+03 | 8.978173e-03\n",
      "076 | 3.761573e+03 | 7.354164e-03\n",
      "077 | 3.742054e+03 | 7.316002e-03\n",
      "078 | 3.752839e+03 | 7.337087e-03\n",
      "079 | 3.639855e+03 | 7.116194e-03\n",
      "080 | 4.169797e+03 | 8.152272e-03\n",
      "081 | 3.920586e+03 | 7.665046e-03\n",
      "082 | 3.841079e+03 | 7.509604e-03\n",
      "083 | 3.499544e+03 | 6.841877e-03\n",
      "084 | 3.620926e+03 | 7.079187e-03\n",
      "085 | 3.739047e+03 | 7.310124e-03\n",
      "086 | 3.370437e+03 | 6.589462e-03\n",
      "087 | 3.390188e+03 | 6.628078e-03\n",
      "088 | 2.997145e+03 | 5.859648e-03\n",
      "089 | 3.154428e+03 | 6.167148e-03\n",
      "090 | 3.026601e+03 | 5.917237e-03\n",
      "091 | 3.095031e+03 | 6.051023e-03\n",
      "092 | 3.031503e+03 | 5.926820e-03\n",
      "093 | 2.990955e+03 | 5.847546e-03\n",
      "094 | 2.910655e+03 | 5.690554e-03\n",
      "095 | 2.917821e+03 | 5.704564e-03\n",
      "096 | 2.849459e+03 | 5.570910e-03\n",
      "097 | 2.790159e+03 | 5.454975e-03\n",
      "098 | 2.834034e+03 | 5.540754e-03\n",
      "099 | 2.597383e+03 | 5.078082e-03\n",
      "100 | 2.651033e+03 | 5.182973e-03\n",
      "101 | 2.930481e+03 | 5.729315e-03\n",
      "102 | 2.809059e+03 | 5.491925e-03\n",
      "103 | 2.679017e+03 | 5.237682e-03\n",
      "104 | 2.770256e+03 | 5.416063e-03\n",
      "105 | 2.522759e+03 | 4.932187e-03\n",
      "106 | 2.462913e+03 | 4.815183e-03\n",
      "107 | 2.941017e+03 | 5.749914e-03\n",
      "108 | 2.592206e+03 | 5.067961e-03\n",
      "109 | 2.470204e+03 | 4.829437e-03\n",
      "110 | 2.444087e+03 | 4.778377e-03\n",
      "111 | 2.270282e+03 | 4.438574e-03\n",
      "112 | 2.174422e+03 | 4.251162e-03\n",
      "113 | 2.225588e+03 | 4.351194e-03\n",
      "114 | 2.331549e+03 | 4.558356e-03\n",
      "115 | 2.148742e+03 | 4.200954e-03\n",
      "116 | 2.161673e+03 | 4.226236e-03\n",
      "117 | 2.311031e+03 | 4.518243e-03\n",
      "118 | 2.171367e+03 | 4.245189e-03\n",
      "119 | 2.050473e+03 | 4.008831e-03\n",
      "120 | 2.413351e+03 | 4.718286e-03\n",
      "121 | 2.214503e+03 | 4.329524e-03\n",
      "122 | 2.022885e+03 | 3.954896e-03\n",
      "123 | 2.016261e+03 | 3.941945e-03\n",
      "124 | 1.943252e+03 | 3.799207e-03\n",
      "125 | 2.083665e+03 | 4.073724e-03\n",
      "126 | 2.175986e+03 | 4.254219e-03\n",
      "127 | 1.970797e+03 | 3.853060e-03\n",
      "128 | 1.976321e+03 | 3.863859e-03\n",
      "129 | 1.771830e+03 | 3.464064e-03\n",
      "130 | 1.812815e+03 | 3.544192e-03\n",
      "131 | 1.820371e+03 | 3.558965e-03\n",
      "132 | 1.726810e+03 | 3.376046e-03\n",
      "133 | 1.693639e+03 | 3.311194e-03\n",
      "134 | 1.739775e+03 | 3.401394e-03\n",
      "135 | 1.684662e+03 | 3.293643e-03\n",
      "136 | 1.658844e+03 | 3.243167e-03\n",
      "137 | 1.592375e+03 | 3.113214e-03\n",
      "138 | 1.835606e+03 | 3.588750e-03\n",
      "139 | 1.790083e+03 | 3.499749e-03\n",
      "140 | 1.556123e+03 | 3.042339e-03\n",
      "141 | 1.557256e+03 | 3.044555e-03\n",
      "142 | 1.546235e+03 | 3.023009e-03\n",
      "143 | 1.458545e+03 | 2.851567e-03\n",
      "144 | 1.540324e+03 | 3.011451e-03\n",
      "145 | 1.680392e+03 | 3.285294e-03\n",
      "146 | 1.536594e+03 | 3.004159e-03\n",
      "147 | 1.583092e+03 | 3.095067e-03\n",
      "148 | 1.615741e+03 | 3.158898e-03\n",
      "149 | 1.524671e+03 | 2.980848e-03\n",
      "150 | 1.393802e+03 | 2.724989e-03\n",
      "151 | 1.386807e+03 | 2.711314e-03\n",
      "152 | 1.473017e+03 | 2.879861e-03\n",
      "153 | 1.420379e+03 | 2.776950e-03\n",
      "154 | 1.405038e+03 | 2.746956e-03\n",
      "155 | 1.415130e+03 | 2.766687e-03\n",
      "156 | 1.500052e+03 | 2.932715e-03\n",
      "157 | 1.437937e+03 | 2.811277e-03\n",
      "158 | 1.321874e+03 | 2.584364e-03\n",
      "159 | 1.318605e+03 | 2.577974e-03\n",
      "160 | 1.221152e+03 | 2.387445e-03\n",
      "161 | 1.196327e+03 | 2.338910e-03\n",
      "162 | 1.267819e+03 | 2.478684e-03\n",
      "163 | 1.140271e+03 | 2.229316e-03\n",
      "164 | 1.142942e+03 | 2.234539e-03\n",
      "165 | 1.123453e+03 | 2.196437e-03\n",
      "166 | 1.093794e+03 | 2.138451e-03\n",
      "167 | 1.113502e+03 | 2.176982e-03\n",
      "168 | 1.255284e+03 | 2.454176e-03\n",
      "169 | 1.162691e+03 | 2.273150e-03\n",
      "170 | 1.039534e+03 | 2.032369e-03\n",
      "171 | 1.116786e+03 | 2.183401e-03\n",
      "172 | 1.140129e+03 | 2.229040e-03\n",
      "173 | 1.001940e+03 | 1.958869e-03\n",
      "174 | 1.123069e+03 | 2.195686e-03\n",
      "175 | 9.637046e+02 | 1.884116e-03\n",
      "176 | 9.221338e+02 | 1.802842e-03\n",
      "177 | 1.053622e+03 | 2.059912e-03\n",
      "178 | 9.263473e+02 | 1.811080e-03\n",
      "179 | 9.064513e+02 | 1.772182e-03\n",
      "180 | 1.023518e+03 | 2.001056e-03\n",
      "181 | 9.024820e+02 | 1.764421e-03\n",
      "182 | 8.305406e+02 | 1.623770e-03\n",
      "183 | 8.179557e+02 | 1.599166e-03\n",
      "184 | 8.372497e+02 | 1.636887e-03\n",
      "185 | 9.200435e+02 | 1.798755e-03\n",
      "186 | 8.921235e+02 | 1.744170e-03\n",
      "187 | 8.672603e+02 | 1.695560e-03\n",
      "188 | 9.011667e+02 | 1.761850e-03\n",
      "189 | 7.996215e+02 | 1.563321e-03\n",
      "190 | 7.955214e+02 | 1.555305e-03\n",
      "191 | 8.937448e+02 | 1.747339e-03\n",
      "192 | 7.887293e+02 | 1.542026e-03\n",
      "193 | 8.478717e+02 | 1.657654e-03\n",
      "194 | 7.284606e+02 | 1.424196e-03\n",
      "195 | 6.941321e+02 | 1.357081e-03\n",
      "196 | 7.226973e+02 | 1.412928e-03\n",
      "197 | 7.143846e+02 | 1.396677e-03\n",
      "198 | 6.771707e+02 | 1.323920e-03\n",
      "199 | 6.287504e+02 | 1.229255e-03\n",
      "200 | 6.685065e+02 | 1.306981e-03\n",
      "201 | 7.314390e+02 | 1.430019e-03\n",
      "202 | 6.663770e+02 | 1.302818e-03\n",
      "203 | 7.116653e+02 | 1.391360e-03\n",
      "204 | 7.606404e+02 | 1.487110e-03\n",
      "205 | 6.826119e+02 | 1.334559e-03\n",
      "206 | 6.096235e+02 | 1.191861e-03\n",
      "207 | 6.499638e+02 | 1.270729e-03\n",
      "208 | 6.335443e+02 | 1.238628e-03\n",
      "209 | 5.578074e+02 | 1.090556e-03\n",
      "210 | 6.793473e+02 | 1.328176e-03\n",
      "211 | 6.249264e+02 | 1.221779e-03\n",
      "212 | 5.853318e+02 | 1.144369e-03\n",
      "213 | 5.800467e+02 | 1.134036e-03\n",
      "214 | 5.473311e+02 | 1.070074e-03\n",
      "215 | 5.475705e+02 | 1.070542e-03\n",
      "216 | 5.203511e+02 | 1.017326e-03\n",
      "217 | 5.302458e+02 | 1.036671e-03\n",
      "218 | 5.379623e+02 | 1.051757e-03\n",
      "219 | 5.200437e+02 | 1.016725e-03\n",
      "220 | 5.030940e+02 | 9.835872e-04\n",
      "221 | 5.498988e+02 | 1.075094e-03\n",
      "222 | 5.601029e+02 | 1.095044e-03\n",
      "223 | 5.241960e+02 | 1.024843e-03\n",
      "224 | 4.488117e+02 | 8.774612e-04\n",
      "225 | 4.911348e+02 | 9.602061e-04\n",
      "226 | 4.836131e+02 | 9.455006e-04\n",
      "227 | 4.583090e+02 | 8.960291e-04\n",
      "228 | 4.475151e+02 | 8.749263e-04\n",
      "229 | 4.288683e+02 | 8.384704e-04\n",
      "230 | 3.996750e+02 | 7.813951e-04\n",
      "231 | 4.195827e+02 | 8.203162e-04\n",
      "232 | 4.841546e+02 | 9.465594e-04\n",
      "233 | 4.515609e+02 | 8.828361e-04\n",
      "234 | 4.157864e+02 | 8.128942e-04\n",
      "235 | 4.093479e+02 | 8.003064e-04\n",
      "236 | 3.750341e+02 | 7.332204e-04\n",
      "237 | 3.948057e+02 | 7.718754e-04\n",
      "238 | 4.662575e+02 | 9.115692e-04\n",
      "239 | 4.378750e+02 | 8.560791e-04\n",
      "240 | 3.567700e+02 | 6.975126e-04\n",
      "241 | 4.078790e+02 | 7.974347e-04\n",
      "242 | 3.869355e+02 | 7.564886e-04\n",
      "243 | 3.447727e+02 | 6.740570e-04\n",
      "244 | 3.715482e+02 | 7.264052e-04\n",
      "245 | 3.809110e+02 | 7.447102e-04\n",
      "246 | 3.507629e+02 | 6.857684e-04\n",
      "247 | 3.483614e+02 | 6.810731e-04\n",
      "248 | 3.690798e+02 | 7.215792e-04\n",
      "249 | 3.448012e+02 | 6.741128e-04\n",
      "250 | 3.231570e+02 | 6.317967e-04\n",
      "251 | 3.500278e+02 | 6.843312e-04\n",
      "252 | 3.325049e+02 | 6.500725e-04\n",
      "253 | 3.302174e+02 | 6.456003e-04\n",
      "254 | 3.814776e+02 | 7.458180e-04\n",
      "255 | 3.026988e+02 | 5.917992e-04\n",
      "256 | 3.166700e+02 | 6.191140e-04\n",
      "257 | 3.515273e+02 | 6.872627e-04\n",
      "258 | 3.500299e+02 | 6.843352e-04\n",
      "259 | 2.804587e+02 | 5.483182e-04\n",
      "260 | 2.899302e+02 | 5.668358e-04\n",
      "261 | 2.696551e+02 | 5.271964e-04\n",
      "262 | 2.797874e+02 | 5.470058e-04\n",
      "263 | 2.987612e+02 | 5.841010e-04\n",
      "264 | 2.690458e+02 | 5.260052e-04\n",
      "265 | 2.819765e+02 | 5.512856e-04\n",
      "266 | 2.666102e+02 | 5.212433e-04\n",
      "267 | 2.460022e+02 | 4.809531e-04\n",
      "268 | 2.575407e+02 | 5.035118e-04\n",
      "269 | 2.789703e+02 | 5.454083e-04\n",
      "270 | 2.483721e+02 | 4.855866e-04\n",
      "271 | 2.426064e+02 | 4.743140e-04\n",
      "272 | 2.682641e+02 | 5.244768e-04\n",
      "273 | 2.584767e+02 | 5.053417e-04\n",
      "274 | 2.773989e+02 | 5.423361e-04\n",
      "275 | 2.397673e+02 | 4.687635e-04\n",
      "276 | 2.595830e+02 | 5.075047e-04\n",
      "277 | 2.638200e+02 | 5.157882e-04\n",
      "278 | 2.174265e+02 | 4.250854e-04\n",
      "279 | 2.196492e+02 | 4.294310e-04\n",
      "280 | 2.357946e+02 | 4.609965e-04\n",
      "281 | 2.272840e+02 | 4.443576e-04\n",
      "282 | 2.091410e+02 | 4.088866e-04\n",
      "283 | 2.198714e+02 | 4.298655e-04\n",
      "284 | 2.271234e+02 | 4.440436e-04\n",
      "285 | 2.043404e+02 | 3.995011e-04\n",
      "286 | 2.086157e+02 | 4.078597e-04\n",
      "287 | 2.178473e+02 | 4.259082e-04\n",
      "288 | 1.985789e+02 | 3.882369e-04\n",
      "289 | 2.009169e+02 | 3.928079e-04\n",
      "290 | 2.136066e+02 | 4.176172e-04\n",
      "291 | 1.886655e+02 | 3.688555e-04\n",
      "292 | 1.904793e+02 | 3.724015e-04\n",
      "293 | 2.427159e+02 | 4.745281e-04\n",
      "294 | 1.958670e+02 | 3.829349e-04\n",
      "295 | 1.985047e+02 | 3.880919e-04\n",
      "296 | 2.027400e+02 | 3.963721e-04\n",
      "297 | 2.174461e+02 | 4.251238e-04\n",
      "298 | 1.703696e+02 | 3.330856e-04\n",
      "299 | 1.695023e+02 | 3.313900e-04\n",
      "300 | 1.888966e+02 | 3.693073e-04\n",
      "301 | 1.794991e+02 | 3.509345e-04\n",
      "302 | 1.647834e+02 | 3.221641e-04\n",
      "303 | 1.734202e+02 | 3.390498e-04\n",
      "304 | 1.774887e+02 | 3.470040e-04\n",
      "305 | 1.646722e+02 | 3.219468e-04\n",
      "306 | 1.583468e+02 | 3.095802e-04\n",
      "307 | 1.545787e+02 | 3.022132e-04\n",
      "308 | 1.538682e+02 | 3.008241e-04\n",
      "309 | 1.581233e+02 | 3.091431e-04\n",
      "310 | 1.563323e+02 | 3.056416e-04\n",
      "311 | 1.489259e+02 | 2.911615e-04\n",
      "312 | 1.829443e+02 | 3.576701e-04\n",
      "313 | 1.526100e+02 | 2.983641e-04\n",
      "314 | 1.520645e+02 | 2.972976e-04\n",
      "315 | 1.632336e+02 | 3.191342e-04\n",
      "316 | 1.685374e+02 | 3.295035e-04\n",
      "317 | 1.480558e+02 | 2.894605e-04\n",
      "318 | 1.351350e+02 | 2.641993e-04\n",
      "319 | 1.303937e+02 | 2.549297e-04\n",
      "320 | 1.291941e+02 | 2.525843e-04\n",
      "321 | 1.425532e+02 | 2.787024e-04\n",
      "322 | 1.314147e+02 | 2.569259e-04\n",
      "323 | 1.201322e+02 | 2.348677e-04\n",
      "324 | 1.317294e+02 | 2.575410e-04\n",
      "325 | 1.260070e+02 | 2.463533e-04\n",
      "326 | 1.139105e+02 | 2.227038e-04\n",
      "327 | 1.253702e+02 | 2.451083e-04\n",
      "328 | 1.235840e+02 | 2.416162e-04\n",
      "329 | 1.268761e+02 | 2.480525e-04\n",
      "330 | 1.415388e+02 | 2.767192e-04\n",
      "331 | 1.272423e+02 | 2.487685e-04\n",
      "332 | 1.112621e+02 | 2.175258e-04\n",
      "333 | 1.161436e+02 | 2.270695e-04\n",
      "334 | 1.241921e+02 | 2.428051e-04\n",
      "335 | 1.267614e+02 | 2.478283e-04\n",
      "336 | 1.100348e+02 | 2.151264e-04\n",
      "337 | 1.131405e+02 | 2.211983e-04\n",
      "338 | 1.071753e+02 | 2.095359e-04\n",
      "339 | 1.066717e+02 | 2.085513e-04\n",
      "340 | 1.123632e+02 | 2.196787e-04\n",
      "341 | 1.070607e+02 | 2.093118e-04\n",
      "342 | 9.751717e+01 | 1.906535e-04\n",
      "343 | 1.038773e+02 | 2.030881e-04\n",
      "344 | 1.029123e+02 | 2.012014e-04\n",
      "345 | 8.987791e+01 | 1.757182e-04\n",
      "346 | 1.013250e+02 | 1.980982e-04\n",
      "347 | 9.541564e+01 | 1.865449e-04\n",
      "348 | 8.990189e+01 | 1.757651e-04\n",
      "349 | 1.010613e+02 | 1.975826e-04\n",
      "350 | 1.008738e+02 | 1.972160e-04\n",
      "351 | 9.240756e+01 | 1.806638e-04\n",
      "352 | 9.990561e+01 | 1.953231e-04\n",
      "353 | 9.251505e+01 | 1.808740e-04\n",
      "354 | 8.251297e+01 | 1.613192e-04\n",
      "355 | 1.072034e+02 | 2.095909e-04\n",
      "356 | 9.380055e+01 | 1.833873e-04\n",
      "357 | 7.615796e+01 | 1.488946e-04\n",
      "358 | 8.819910e+01 | 1.724360e-04\n",
      "359 | 8.540937e+01 | 1.669819e-04\n",
      "360 | 7.273466e+01 | 1.422018e-04\n",
      "361 | 8.691430e+01 | 1.699241e-04\n",
      "362 | 7.887929e+01 | 1.542150e-04\n",
      "363 | 7.198998e+01 | 1.407459e-04\n",
      "364 | 8.306338e+01 | 1.623953e-04\n",
      "365 | 7.464616e+01 | 1.459390e-04\n",
      "366 | 6.715678e+01 | 1.312966e-04\n",
      "367 | 7.672105e+01 | 1.499955e-04\n",
      "368 | 7.317628e+01 | 1.430652e-04\n",
      "369 | 7.672720e+01 | 1.500075e-04\n",
      "370 | 7.712390e+01 | 1.507831e-04\n",
      "371 | 6.737289e+01 | 1.317192e-04\n",
      "372 | 6.510111e+01 | 1.272777e-04\n",
      "373 | 7.154808e+01 | 1.398820e-04\n",
      "374 | 7.036414e+01 | 1.375673e-04\n",
      "375 | 6.967848e+01 | 1.362268e-04\n",
      "376 | 6.965442e+01 | 1.361797e-04\n",
      "377 | 6.683415e+01 | 1.306659e-04\n",
      "378 | 6.924582e+01 | 1.353809e-04\n",
      "379 | 6.142042e+01 | 1.200816e-04\n",
      "380 | 6.290722e+01 | 1.229884e-04\n",
      "381 | 6.191621e+01 | 1.210509e-04\n",
      "382 | 5.731794e+01 | 1.120610e-04\n",
      "383 | 5.792171e+01 | 1.132414e-04\n",
      "384 | 5.766839e+01 | 1.127461e-04\n",
      "385 | 6.139891e+01 | 1.200396e-04\n",
      "386 | 5.550890e+01 | 1.085241e-04\n",
      "387 | 6.174563e+01 | 1.207174e-04\n",
      "388 | 6.401674e+01 | 1.251576e-04\n",
      "389 | 5.965764e+01 | 1.166353e-04\n",
      "390 | 5.199747e+01 | 1.016590e-04\n",
      "391 | 5.546453e+01 | 1.084374e-04\n",
      "392 | 5.453132e+01 | 1.066129e-04\n",
      "393 | 4.890766e+01 | 9.561822e-05\n",
      "迭代 393 次收敛，耗时 5.800s\n",
      "\n",
      "求解相对误差: 1.414921e-02\n",
      "支撑集一致性: 0.000000e+00 (应接近0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# （接你提供的代码）\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "XTX = H_A  # X⊤X矩阵\n",
    "W_hat = new_W.to(torch.float32).cuda()  # 剪枝后的初始解（带支撑集）\n",
    "S = (W_hat != 0).to(torch.float32).cuda()  # 支撑集掩码（非零元素位置）\n",
    "W_0 = ora_W.to(torch.float32).cuda()  # 目标解（论文中的cW）\n",
    "\n",
    "\n",
    "def pcg_vectorized(XTX, W0, S, max_iter=50, rtol=1e-4, atol=1e-6, verbose=False):\n",
    "\n",
    "    device = XTX.device\n",
    "    n, m = W0.shape  # n: 特征维度，m: 批量大小（如输出维度）\n",
    "\n",
    "    # 初始化解W（初始猜测为W_hat，即带支撑集的初始解）\n",
    "    W = W_hat.clone()\n",
    "\n",
    "    # 步骤1: 计算初始残差 R0 = X⊤X (W0 - W)\n",
    "    R = XTX @ (W0 - W)\n",
    "\n",
    "    # 步骤2: 投影残差到支撑集S（支撑集外元素置零）\n",
    "    R = R * S\n",
    "\n",
    "    # 步骤3: 初始化预条件子M = Diag(XTX)，M_inv = Diag(1/diag(XTX))\n",
    "    diag_XTX = torch.diag(XTX).unsqueeze(1)  # 扩展为(n×1)，方便广播到(n×m)\n",
    "    M_inv = 1.0 / (diag_XTX + 1e-10)  # 避免除零\n",
    "    Z = M_inv * R  # Z0 = M^{-1} R0\n",
    "    P = Z.clone()  # P0 = Z0\n",
    "\n",
    "    # 收敛判断阈值\n",
    "    R0_norm = torch.norm(R)\n",
    "    stopping_thresh = torch.max(rtol * R0_norm, torch.tensor(atol, device=device))\n",
    "\n",
    "    # 迭代信息\n",
    "    optimal = False\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"初始化残差范数: {R0_norm:.6e}，收敛阈值: {stopping_thresh:.6e}\")\n",
    "        print(\"%03s | %10s | %10s\" % (\"迭代\", \"残差范数\", \"相对误差\"))\n",
    "\n",
    "    for t in range(max_iter):\n",
    "        # 步骤5: 计算αt = Tr(Rt⊤ Zt) / Tr(Pt⊤ X⊤X Pt)\n",
    "        # Tr(A⊤B) = 元素乘积之和（对批量矩阵按列求和后累加）\n",
    "        numerator = torch.sum(R * Z)  # 等价于Tr(R⊤Z)\n",
    "        XTX_P = XTX @ P\n",
    "        denominator = torch.sum(P * XTX_P)  # 等价于Tr(P⊤ X⊤X P)\n",
    "        alpha = numerator / (denominator + 1e-10)  # 避免除零\n",
    "\n",
    "        # 步骤6: 更新解 Wt+1 = Wt + αt Pt\n",
    "        W = W + alpha * P\n",
    "\n",
    "        # 步骤7: 更新残差 Rt+1 = Rt - αt X⊤X Pt\n",
    "        R = R - alpha * XTX_P\n",
    "\n",
    "        # 步骤8: 投影残差到支撑集S\n",
    "        R = R * S\n",
    "\n",
    "        # 检查收敛\n",
    "        current_norm = torch.norm(R)\n",
    "        rel_error = current_norm / (R0_norm + 1e-10)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"%03d | %10.6e | %10.6e\" % (t+1, current_norm.item(), rel_error.item()))\n",
    "\n",
    "        if current_norm < stopping_thresh or rel_error < rtol:\n",
    "            optimal = True\n",
    "            break\n",
    "\n",
    "        # 步骤9: 计算Zt+1 = M^{-1} Rt+1\n",
    "        Z = M_inv * R\n",
    "\n",
    "        # 步骤13: 计算βt = Tr(Rt+1⊤ Zt+1) / Tr(Rt⊤ Zt)\n",
    "        beta_numerator = torch.sum(R * Z)\n",
    "        beta_denominator = numerator + 1e-10  # 复用之前的numerator=Tr(Rt⊤ Zt)\n",
    "        beta = beta_numerator / beta_denominator\n",
    "\n",
    "        # 步骤14: 更新搜索方向 Pt+1 = Zt+1 + βt Pt\n",
    "        P = Z + beta * P\n",
    "\n",
    "    # 迭代结束\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    if verbose:\n",
    "        if optimal:\n",
    "            print(f\"迭代 {t+1} 次收敛，耗时 {total_time:.3f}s\")\n",
    "        else:\n",
    "            print(f\"达到最大迭代次数 {max_iter}，耗时 {total_time:.3f}s，最终残差: {current_norm:.6e}\")\n",
    "\n",
    "    info = {\n",
    "        \"niter\": t+1,\n",
    "        \"optimal\": optimal,\n",
    "        \"final_residual\": current_norm.item(),\n",
    "        \"time\": total_time\n",
    "    }\n",
    "    return W, info\n",
    "\n",
    "\n",
    "# 调用PCG求解\n",
    "print(\"\\n===== 运行论文中的PCG算法 =====\")\n",
    "W_final, pcg_info = pcg_vectorized(\n",
    "    XTX=XTX,\n",
    "    W0=W_0,\n",
    "    S=S,\n",
    "    max_iter=500,\n",
    "    rtol=1e-4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 验证结果：检查 X⊤X W ≈ X⊤X W0\n",
    "XTX_W = XTX @ W_final\n",
    "XTX_W0 = XTX @ W_0\n",
    "error = torch.norm(XTX_W - XTX_W0, p='fro') / torch.norm(XTX_W0, p='fro')\n",
    "print(f\"\\n求解相对误差: {error.item():.6e}\")\n",
    "print(f\"支撑集一致性: {(W_final * (1 - S)).abs().max().item():.6e} (应接近0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb974e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(1.7285, device='cuda:0', dtype=torch.float16) tensor(0.1083, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(1.7207, device='cuda:0', dtype=torch.float16) tensor(0.1102, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(2.0273, device='cuda:0', dtype=torch.float16) tensor(0.1140, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(1.6006, device='cuda:0', dtype=torch.float16) tensor(0.1121, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(1.6855, device='cuda:0', dtype=torch.float16) tensor(0.1089, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(1.6328, device='cuda:0', dtype=torch.float16) tensor(0.1095, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "W_final = W_final.to(torch.float16)\n",
    "print((W_final == 0).sum() / W_final.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_final = W_final - M.to(torch.float16) * W_final\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_final @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_final @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2654a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.3000, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [00:03<00:01, 41.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5771, device='cuda:0', dtype=torch.float16) tensor(0.0676, device='cuda:0', dtype=torch.float16)\n",
      "Converged at iteration 150\n",
      "tensor(0.0898, device='cuda:0')\n",
      "time 3.6276891231536865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "\n",
    "lambda_value = []\n",
    "lambda_abs = []\n",
    "alphas = []\n",
    "\n",
    "out = []\n",
    "\n",
    "def alpha_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    # if k >= max_iter:\n",
    "    #     return alpha_end\n",
    "    ratio = k / max_iter\n",
    "    return alpha_end - (alpha_end - alpha_start) * np.exp(-gamma * ratio)\n",
    "\n",
    "def delta_schedule_exp(k, max_iter=100, alpha_start=0.9, alpha_end=0.99, gamma=5.0):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end - alpha_start\n",
    "    else:\n",
    "        return alpha_schedule_exp(k, max_iter, alpha_start, alpha_end, gamma) - alpha_start\n",
    "\n",
    "def alpha_schedule(k, max_iter=100, alpha_start=0.9, alpha_end=0.99):\n",
    "    if k >= max_iter:\n",
    "        return alpha_end\n",
    "    else:\n",
    "        return alpha_start + (alpha_end - alpha_start) * (k / max_iter)\n",
    "\n",
    "def dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, epsilon=1e-1, max_iter=10, lambda_zero=False, percdamp=.01):\n",
    "    M = M.to(torch.float32)\n",
    "    H_A = H_A.to(torch.float32)\n",
    "    H_B = H_B.to(torch.float32)\n",
    "    W_old = W_old.to(torch.float32)\n",
    "    \n",
    "    # 初始化 W 和 Lambda\n",
    "    W = W_old.clone()\n",
    "    if lambda_zero:\n",
    "        Lambda = torch.zeros_like(W)\n",
    "    # else:\n",
    "    #     term1 = beta * (torch.mm(W, H_A) - H_B)\n",
    "    #     term2 = alpha * (W - W_old)\n",
    "    #     Lambda = -M * (term1 + term2)\n",
    "    I = torch.eye(H_A.shape[0], device=H_A.device)\n",
    "    A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "    A_start = torch.linalg.cholesky(A)\n",
    "    A_start = torch.cholesky_inverse(A_start)\n",
    "    \n",
    "    A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "    A2 = (-A1) @ (I - H_A) @ A_start\n",
    "    A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "    alpha_start = alpha\n",
    "    for k in tqdm(range(max_iter)):\n",
    "        alpha = alpha_schedule_exp(k, alpha_start=alpha_start)\n",
    "        delta = alpha - alpha_start\n",
    "\n",
    "        if k % 20 == 0 and k < 100:\n",
    "            alpha_start = alpha\n",
    "\n",
    "            A = (1-alpha) * H_A + alpha * I\n",
    "\n",
    "            A_start = torch.linalg.cholesky(A)\n",
    "            A_start = torch.cholesky_inverse(A_start)\n",
    "            \n",
    "            A1 = -(A_start @ (I - H_A) @ A_start)\n",
    "            A2 = (-A1) @ (I - H_A) @ A_start\n",
    "            A3 = (-A2) @ (I - H_A) @ A_start\n",
    "\n",
    "\n",
    "\n",
    "        W_prev = W.clone()\n",
    "\n",
    "        B = (1-alpha-delta) * H_B + (alpha + delta) * W_prev\n",
    "        B = B - Lambda\n",
    "\n",
    "        # A_inv = (1-alpha-delta) * H_A + (alpha + delta) * I\n",
    "        # A_inv = torch.linalg.cholesky(A_inv)\n",
    "        # A_inv = torch.cholesky_inverse(A_inv)\n",
    "\n",
    "        W = B @ (A_start + delta * A1 + delta**2 * A2 + delta**3 * A3)\n",
    "        # W = B @ A_inv\n",
    "\n",
    "        # print(\"二阶近似误差\",torch.norm(((A_start + delta * A1 + delta**2 * A2) - A_inv)))\n",
    "\n",
    "\n",
    "        Lambda = Lambda + rho * (M * W)\n",
    "\n",
    "        lambda_value.append(Lambda.mean().item())\n",
    "        lambda_abs.append(Lambda.abs().mean().item())\n",
    "        out.append((M * W).mean().item())\n",
    "        # W = W - M * W\n",
    "        # 收敛判断\n",
    "\n",
    "        if k % 50 == 0 and k > 0:\n",
    "            \n",
    "\n",
    "            # print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "            # print(torch.norm(W - W_prev))\n",
    "            if torch.norm(W - W_prev) < epsilon:\n",
    "                print( (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().max(), (W.to(torch.float16) @ test_inps[0].cuda() - test_outs[0][0].cuda()).abs().mean())\n",
    "                print(f\"Converged at iteration {k}\")\n",
    "                print(torch.norm(W - W_prev))\n",
    "                break\n",
    "\n",
    "        # if k % 10 == 0 :\n",
    "        #     print(torch.norm(W - W_prev))\n",
    "    W = W - M * W\n",
    "    return W.to(torch.float16)\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = dual_ascent_method10(H_A, H_B, W_old, M, beta, alpha, gama, rho, lambda_zero=True, max_iter=200)\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3805f360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2969, device='cuda:0') tensor(0.0003, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "B = torch.matmul(ora_W.to(torch.float32), H_A).to(torch.float32).cuda()\n",
    "print((B-H_B).abs().max(), (B-H_B).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d66262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(0.5762, device='cuda:0', dtype=torch.float16) tensor(0.0679, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(0.8398, device='cuda:0', dtype=torch.float16) tensor(0.0696, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(0.6514, device='cuda:0', dtype=torch.float16) tensor(0.0791, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(0.5791, device='cuda:0', dtype=torch.float16) tensor(0.0734, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0706, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(0.6953, device='cuda:0', dtype=torch.float16) tensor(0.0701, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a73f2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7000, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(0.5762, device='cuda:0', dtype=torch.float16) tensor(0.0679, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(0.8398, device='cuda:0', dtype=torch.float16) tensor(0.0696, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(0.6514, device='cuda:0', dtype=torch.float16) tensor(0.0791, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(0.5791, device='cuda:0', dtype=torch.float16) tensor(0.0734, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0706, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(0.6953, device='cuda:0', dtype=torch.float16) tensor(0.0701, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e49dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.3000, device='cuda:0')\n",
      "time 40.19992709159851\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "def cg_batch(A, B, A_supp, M_bmm=None, X0=None, rtol=1e-3, atol=0., maxiter=None, verbose=False):\n",
    "    \"\"\"Solves a batch of PD matrix linear systems using the preconditioned CG algorithm.\n",
    "\n",
    "    This function solves matrix linear systems of the form\n",
    "\n",
    "        A X = B,  \n",
    "\n",
    "    where A is a n x n positive definite matrix and B is a n x m matrix,\n",
    "    and X is the n x m matrix representing the solution for the ith system.\n",
    "\n",
    "    Args:\n",
    "        A_bmm: A callable that performs a batch matrix multiply of A and a n x m matrix.\n",
    "        B: A n x m matrix representing the right hand sides.\n",
    "        M_bmm: (optional) A callable that performs a batch matrix multiply of the preconditioning\n",
    "        matrices M and a n x m matrix. (default=identity matrix)\n",
    "        X0: (optional) Initial guess for X, defaults to M_bmm(B). (default=None)\n",
    "        rtol: (optional) Relative tolerance for norm of residual. (default=1e-3)\n",
    "        atol: (optional) Absolute tolerance for norm of residual. (default=0)\n",
    "        maxiter: (optional) Maximum number of iterations to perform. (default=5*n)\n",
    "        verbose: (optional) Whether or not to print status messages. (default=False)\n",
    "    \"\"\"\n",
    "    error_list = np.zeros((maxiter,))\n",
    "    n, m = B.shape\n",
    "\n",
    "    if M_bmm is None:\n",
    "        M_bmm = lambda x: x\n",
    "    if X0 is None:\n",
    "        X0 = M_bmm(B)\n",
    "    if maxiter is None:\n",
    "        maxiter = 5 * n\n",
    "\n",
    "    assert B.shape == (n, m)\n",
    "    assert X0.shape == (n, m)\n",
    "    assert rtol > 0 or atol > 0\n",
    "    assert isinstance(maxiter, int)\n",
    "\n",
    "    X_k = X0\n",
    "\n",
    "    R_k = B - A @ X_k\n",
    "    R_k = R_k * A_supp\n",
    "\n",
    "    Z_k = M_bmm(R_k)\n",
    "\n",
    "    P_k = torch.zeros_like(Z_k)\n",
    "\n",
    "    P_k1 = P_k\n",
    "    R_k1 = R_k\n",
    "    R_k2 = R_k\n",
    "    X_k1 = X0\n",
    "    Z_k1 = Z_k\n",
    "    Z_k2 = Z_k\n",
    "\n",
    "    B_norm = torch.norm(B, dim=1)\n",
    "    stopping_matrix = torch.max(rtol*B_norm, atol*torch.ones_like(B_norm))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"%03s | %010s %06s\" % (\"it\", \"dist\", \"it/s\"))\n",
    "\n",
    "    optimal = False\n",
    "    start = time.perf_counter()\n",
    "    for k in range(1, maxiter + 1):\n",
    "        start_iter = time.perf_counter()\n",
    "        Z_k = M_bmm(R_k)\n",
    "\n",
    "        if k == 1:\n",
    "            P_k = Z_k\n",
    "            R_k1 = R_k\n",
    "            X_k1 = X_k\n",
    "            Z_k1 = Z_k\n",
    "        else:\n",
    "            R_k2 = R_k1\n",
    "            Z_k2 = Z_k1\n",
    "            P_k1 = P_k\n",
    "            R_k1 = R_k\n",
    "            Z_k1 = Z_k\n",
    "            X_k1 = X_k\n",
    "            denominator = (R_k2 * Z_k2).sum(0)\n",
    "            denominator[denominator == 0] = 1e-8\n",
    "            beta = (R_k1 * Z_k1).sum(0) / denominator\n",
    "            P_k = Z_k1 + beta.unsqueeze(0) * P_k1\n",
    "\n",
    "        denominator = (P_k * (A@P_k)).sum(0)\n",
    "        denominator[denominator == 0] = 1e-8\n",
    "        alpha = (R_k1 * Z_k1).sum(0) / denominator\n",
    "        X_k = X_k1 + alpha.unsqueeze(0) * P_k\n",
    "        R_k = R_k1 - alpha.unsqueeze(0) * (A@P_k)\n",
    "        R_k = R_k * A_supp\n",
    "        end_iter = time.perf_counter()\n",
    "\n",
    "        residual_norm = torch.norm(A@X_k - B, dim=1)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"%03d | %8.4e\" %\n",
    "                    (k, torch.max(residual_norm/B_norm)))\n",
    "\n",
    "        if (residual_norm <= stopping_matrix).all():\n",
    "            optimal = True\n",
    "            break\n",
    "\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    if verbose:\n",
    "        if optimal:\n",
    "            print(\"Terminated in %d steps (optimal). Took %.3f ms.\" %\n",
    "                    (k, (end - start) * 1000))\n",
    "        else:\n",
    "            print(\"Terminated in %d steps (reached maxiter). Took %.3f ms.\" %\n",
    "                    (k, (end - start) * 1000))\n",
    "\n",
    "\n",
    "    info = {\n",
    "        \"niter\": k,\n",
    "        \"optimal\": optimal\n",
    "    }\n",
    "\n",
    "    return X_k\n",
    "\n",
    "\n",
    "test_x = test_inps[0]\n",
    "test_y = test_outs[0][0]\n",
    "\n",
    "H_A = torch.zeros((test_x.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "H_B = torch.zeros((test_y.shape[-2], test_x.shape[-2]), device=\"cuda\")\n",
    "nsamples = 0\n",
    "for i in range(120):\n",
    "    H_A *= nsamples / (nsamples + 1)\n",
    "    H_B *= nsamples / (nsamples + 1)\n",
    "\n",
    "    nsamples += 1\n",
    "\n",
    "    H_A += (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "    H_B += (math.sqrt(2 / nsamples) * test_outs[i][0].cuda().to(torch.float32)) @ (math.sqrt(2 / nsamples) * test_inps[i].cuda().to(torch.float32)).T\n",
    "\n",
    "    \n",
    "\n",
    "M = (new_W == 0).to(torch.float32).to(H_B.device)\n",
    "print(M.device)\n",
    "# M = torch.zeros_like(M).to(torch.float32).cuda()\n",
    "print((M == 0).sum() / M.numel())\n",
    "W_old = new_W.clone().to(H_B.device)\n",
    "beta = 0.01\n",
    "alpha = 0.9\n",
    "gama = 0.0000\n",
    "rho = 1\n",
    "\n",
    "X_norm = torch.diag(H_A).sqrt()\n",
    "\n",
    "H_A = H_A / X_norm\n",
    "H_A = (H_A.T /X_norm).T\n",
    "H_B = torch.matmul(ora_W.to(torch.float32) * X_norm, H_A)\n",
    "B = (W_old * X_norm).T\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "W_update = cg_batch(H_A, H_B, (W_old != 0).to(torch.float32), X0=B, rtol=1e-4, atol=0., maxiter=1000, verbose=False)\n",
    "W_update = W_update.T / X_norm\n",
    "print('time', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6943496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W_update tensor(12.9766, device='cuda:0', dtype=torch.float16) tensor(0.0558, device='cuda:0', dtype=torch.float16)\n",
      "tensor(0.4901, device='cuda:0')\n",
      "new_W tensor(0.2256, device='cuda:0', dtype=torch.float16) tensor(0.0073, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final W_update\", W_update.abs().max(), W_update.abs().mean())\n",
    "print((W_update == 0).sum() / W_update.numel())\n",
    "print(\"new_W\", new_W.abs().max(), new_W.abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "198febc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4901, device='cuda:0')\n",
      "tensor(3.9316, dtype=torch.float16) tensor(0.2715, dtype=torch.float16)\n",
      "0 tensor(42.9062, device='cuda:0', dtype=torch.float16) tensor(1.1484, device='cuda:0', dtype=torch.float16)\n",
      "0 tensor(0.5742, device='cuda:0', dtype=torch.float16) tensor(0.0740, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(5.1016, dtype=torch.float16) tensor(0.2737, dtype=torch.float16)\n",
      "32 tensor(37.8438, device='cuda:0', dtype=torch.float16) tensor(1.1699, device='cuda:0', dtype=torch.float16)\n",
      "32 tensor(0.8164, device='cuda:0', dtype=torch.float16) tensor(0.0758, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.5605, dtype=torch.float16) tensor(0.2708, dtype=torch.float16)\n",
      "63 tensor(49.8750, device='cuda:0', dtype=torch.float16) tensor(1.2998, device='cuda:0', dtype=torch.float16)\n",
      "63 tensor(0.6562, device='cuda:0', dtype=torch.float16) tensor(0.0837, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(3.7148, dtype=torch.float16) tensor(0.2778, dtype=torch.float16)\n",
      "122 tensor(38.3125, device='cuda:0', dtype=torch.float16) tensor(1.2402, device='cuda:0', dtype=torch.float16)\n",
      "122 tensor(0.5972, device='cuda:0', dtype=torch.float16) tensor(0.0773, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.6094, dtype=torch.float16) tensor(0.2725, dtype=torch.float16)\n",
      "123 tensor(51.5938, device='cuda:0', dtype=torch.float16) tensor(1.1807, device='cuda:0', dtype=torch.float16)\n",
      "123 tensor(0.6387, device='cuda:0', dtype=torch.float16) tensor(0.0750, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n",
      "tensor(4.3789, dtype=torch.float16) tensor(0.2661, dtype=torch.float16)\n",
      "124 tensor(38.1875, device='cuda:0', dtype=torch.float16) tensor(1.1943, device='cuda:0', dtype=torch.float16)\n",
      "124 tensor(0.7188, device='cuda:0', dtype=torch.float16) tensor(0.0743, device='cuda:0', dtype=torch.float16)\n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "W_update = W_update.to(torch.float16)\n",
    "print((W_update == 0).sum() / W_update.numel())\n",
    "for i in [0, 32, 63, 122, 123, 124]:\n",
    "    # W_update = W_update - M.to(torch.float16) * W_update\n",
    "    print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(i, (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    print(i, (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (new_W.to(H_B.device) @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # W_update2 = W_update - M.to(torch.float16) * W_update\n",
    "    # print(i, (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().max(), (W_update2 @ test_inps[i].cuda() - test_outs[i][0].cuda()).abs().mean())\n",
    "    # print(\"-----------\")\n",
    "    # print((new_W @ test_inps[i]).abs().max(), (new_W @ test_inps[i]).abs().mean())\n",
    "    # print((W_update2 @ test_inps[i]).abs().max(), (W_update2 @ test_inps[i]).abs().mean())\n",
    "    # print(test_outs[i][0].abs().max(), test_outs[i][0].abs().mean())\n",
    "    print(\"-------------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
