{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f38cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = 4,5,6,7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.3.1+cu121\n",
      "torch 2.3.1+cu121\n",
      "transformers 4.47.1\n",
      "accelerate 0.29.1\n",
      "# of gpus:  4\n",
      "torch 2.3.1+cu121\n",
      "transformers 4.47.1\n",
      "accelerate 0.29.1\n",
      "# of gpus:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 15:25:43.101536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-15 15:25:43.314195: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-15 15:25:43.320661: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-15 15:25:43.320690: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-07-15 15:25:45.296670: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-15 15:25:45.297189: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-12.3/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-07-15 15:25:45.297215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab71fd3be8fb439685e2e331a6380643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 5120])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.0.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.1.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.2.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.3.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.4.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.5.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.6.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.7.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.8.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.9.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.10.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.11.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.12.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.13.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.14.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.15.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.16.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.17.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.18.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.19.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.20.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.21.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.22.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.23.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.24.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.25.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.26.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.27.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.28.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.29.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.30.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.31.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.32.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.33.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.34.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.35.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.36.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.37.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.38.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.39.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.norm.weight torch.Size([5120])\n",
      "lm_head.weight torch.Size([32000, 5120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:53<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# from matmul_had import *\n",
    "# from utils import *\n",
    "import os\n",
    "\n",
    "# 设置只使用特定的 GPU（例如 GPU 0 和 GPU 1）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6,7'\n",
    "\n",
    "# 检查设置是否生效\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "import argparse\n",
    "import os \n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from importlib.metadata import version\n",
    "print('torch', version('torch'))\n",
    "\n",
    "from lib.utils import find_layers\n",
    "from lib.data import prepare_calibration_input\n",
    "from lib.data import get_loaders \n",
    "from lib.eval import eval_ppl, eval_zero_shot\n",
    "from lib.esd_utils import get_esd_metrics\n",
    "\n",
    "from main import get_llm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('torch', version('torch'))\n",
    "print('transformers', version('transformers'))\n",
    "print('accelerate', version('accelerate'))\n",
    "print('# of gpus: ', torch.cuda.device_count())\n",
    "\n",
    "class PruneConfig:\n",
    "    def __init__(self):\n",
    "        self.model = '/h3cstore_ns/jcxie/hf_weights/llama-2-13b-hf'\n",
    "        self.seed = 0\n",
    "        self.nsamples = 128\n",
    "        self.sparsity_ratio = 0.9\n",
    "        self.sparsity_type = \"unstructured\"\n",
    "        self.prune_method = \"sparsegpt_silu_ww\"\n",
    "        self.cache_dir = \"llm_weights\"\n",
    "        self.use_variant = False\n",
    "        self.save = '/h3cstore_ns/jcxie/LISA/wanda-main/ckpt'\n",
    "        self.save_model = None\n",
    "        self.exclude = 'gate_proj'\n",
    "        self.ww_metric = \"alpha_peak\"\n",
    "        self.ww_metric_cache = \"/h3cstore_ns/jcxie/LISA/wanda-main/data/llama2-7b-hf\"\n",
    "        self.epsilon = 0.3\n",
    "        self.mapping_type = \"block_wise\"\n",
    "        self.Hyper_m = 3.0\n",
    "        self.Lamda = 0.20\n",
    "        self.eval_zero_shot = False\n",
    "args = PruneConfig()\n",
    "model = get_llm(args.model, args.cache_dir)\n",
    "model.eval()\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.size())\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "    ratios = None\n",
    "    device = model.hf_device_map[\"lm_head\"]\n",
    "\n",
    "    # print(\"loading calibdation data\")\n",
    "    # dataloader, _ = get_loaders(\"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)\n",
    "    # print(\"dataset loading complete\")\n",
    "    # with torch.no_grad():\n",
    "    #     inps, outs, attention_mask, position_ids = prepare_calibration_input(model, dataloader, device, nsamples=args.nsamples)\n",
    "\n",
    "    layers = model.model.layers\n",
    "\n",
    "    layer_num = len(find_layers(layers))\n",
    "    if ratios is None:\n",
    "        ratios = [args.sparsity_ratio for i in range(layer_num)]\n",
    "    k=0\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "with torch.no_grad():\n",
    "    print('Starting ...')\n",
    "    dataloader, _ = get_loaders(args, \"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)\n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    if \"model.embed_tokens\" in model.hf_device_map:\n",
    "        dev = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros(\n",
    "        (args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
    "    )\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask']\n",
    "            cache['position_ids'] = kwargs['position_ids']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch[0].to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache['attention_mask']\n",
    "    position_ids = cache['position_ids']\n",
    "\n",
    "    print('Ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fb4786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 self_attn.q_proj\n",
      "Pruning ...\n",
      "0 self_attn.k_proj\n",
      "Pruning ...\n",
      "0 self_attn.v_proj\n",
      "Pruning ...\n",
      "0 self_attn.o_proj\n",
      "Pruning ...\n",
      "0 mlp.gate_proj\n",
      "Pruning ...\n",
      "0 mlp.up_proj\n",
      "Pruning ...\n",
      "0 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 1 device 0\n",
      "1 self_attn.q_proj\n",
      "Pruning ...\n",
      "1 self_attn.k_proj\n",
      "Pruning ...\n",
      "1 self_attn.v_proj\n",
      "Pruning ...\n",
      "1 self_attn.o_proj\n",
      "Pruning ...\n",
      "1 mlp.gate_proj\n",
      "Pruning ...\n",
      "1 mlp.up_proj\n",
      "Pruning ...\n",
      "1 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 2 device 0\n",
      "2 self_attn.q_proj\n",
      "Pruning ...\n",
      "2 self_attn.k_proj\n",
      "Pruning ...\n",
      "2 self_attn.v_proj\n",
      "Pruning ...\n",
      "2 self_attn.o_proj\n",
      "Pruning ...\n",
      "2 mlp.gate_proj\n",
      "Pruning ...\n",
      "2 mlp.up_proj\n",
      "Pruning ...\n",
      "2 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 3 device 0\n",
      "3 self_attn.q_proj\n",
      "Pruning ...\n",
      "3 self_attn.k_proj\n",
      "Pruning ...\n",
      "3 self_attn.v_proj\n",
      "Pruning ...\n",
      "3 self_attn.o_proj\n",
      "Pruning ...\n",
      "3 mlp.gate_proj\n",
      "Pruning ...\n",
      "3 mlp.up_proj\n",
      "Pruning ...\n",
      "3 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 4 device 0\n",
      "4 self_attn.q_proj\n",
      "Pruning ...\n",
      "4 self_attn.k_proj\n",
      "Pruning ...\n",
      "4 self_attn.v_proj\n",
      "Pruning ...\n",
      "4 self_attn.o_proj\n",
      "Pruning ...\n",
      "4 mlp.gate_proj\n",
      "Pruning ...\n",
      "4 mlp.up_proj\n",
      "Pruning ...\n",
      "4 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 5 device 0\n",
      "5 self_attn.q_proj\n",
      "Pruning ...\n",
      "5 self_attn.k_proj\n",
      "Pruning ...\n",
      "5 self_attn.v_proj\n",
      "Pruning ...\n",
      "5 self_attn.o_proj\n",
      "Pruning ...\n",
      "5 mlp.gate_proj\n",
      "Pruning ...\n",
      "5 mlp.up_proj\n",
      "Pruning ...\n",
      "5 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 6 device 0\n",
      "6 self_attn.q_proj\n",
      "Pruning ...\n",
      "6 self_attn.k_proj\n",
      "Pruning ...\n",
      "6 self_attn.v_proj\n",
      "Pruning ...\n",
      "6 self_attn.o_proj\n",
      "Pruning ...\n",
      "6 mlp.gate_proj\n",
      "Pruning ...\n",
      "6 mlp.up_proj\n",
      "Pruning ...\n",
      "6 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 7 device 0\n",
      "7 self_attn.q_proj\n",
      "Pruning ...\n",
      "7 self_attn.k_proj\n",
      "Pruning ...\n",
      "7 self_attn.v_proj\n",
      "Pruning ...\n",
      "7 self_attn.o_proj\n",
      "Pruning ...\n",
      "7 mlp.gate_proj\n",
      "Pruning ...\n",
      "7 mlp.up_proj\n",
      "Pruning ...\n",
      "7 mlp.down_proj\n",
      "Pruning ...\n",
      "layer 8 device 0\n",
      "8 self_attn.q_proj\n",
      "Pruning ...\n",
      "8 self_attn.k_proj\n",
      "Pruning ...\n",
      "8 self_attn.v_proj\n",
      "Pruning ...\n",
      "8 self_attn.o_proj\n",
      "Pruning ...\n",
      "8 mlp.gate_proj\n",
      "Pruning ...\n",
      "8 mlp.up_proj\n",
      "Pruning ...\n",
      "8 mlp.down_proj\n",
      "Pruning ...\n"
     ]
    }
   ],
   "source": [
    "prune_n, prune_m = 0, 0\n",
    "if args.sparsity_type != \"unstructured\":\n",
    "    assert args.sparsity_ratio == 0.5, \"sparsity ratio must be 0.5 for structured N:M sparsity\"\n",
    "    prune_n, prune_m = map(int, args.sparsity_type.split(\":\"))\n",
    "    \n",
    "from lib.sparsegpt import SparseGPTV2, SparseGPT\n",
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    \n",
    "    layer_num = len(find_layers(layers))\n",
    "    if ratios is None:\n",
    "        ratios = [args.sparsity_ratio for i in range(layer_num)]\n",
    "    k=0\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if f\"model.layers.{i}\" in model.hf_device_map:\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            print(f\"layer {i} device {dev}\")\n",
    "            if attention_mask is None:\n",
    "                inps, outs, position_ids = inps.to(dev), outs.to(dev), position_ids.to(dev)\n",
    "            else:\n",
    "                inps, outs, attention_mask, position_ids = inps.to(dev), outs.to(dev), attention_mask.to(dev), position_ids.to(dev)\n",
    "\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        gpts = {}\n",
    "        for name in subset:\n",
    "\n",
    "            if name == 'self_attn.v_proj' and i == 8:\n",
    "                gpts[name] = SparseGPTV2(subset[name], have_scaler=True)\n",
    "            else:\n",
    "                gpts[name] = SparseGPT(subset[name])\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                gpts[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        for name in gpts:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "                if attention_mask is None:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), position_ids=position_ids)[0]\n",
    "                else:\n",
    "                    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        if i == 8:\n",
    "            test_inps = gpts['self_attn.v_proj'].inps\n",
    "            test_outs = gpts['self_attn.v_proj'].outs\n",
    "            ora_W = model.model.layers[i].self_attn.v_proj.weight.data\n",
    "            # np.save('/h3cstore_ns/jcxie/LISA/wanda-main/npys/test_admm/ora_W.npy', ora_W.cpu().numpy())\n",
    "\n",
    "        for name in gpts:\n",
    "            print(i, name)\n",
    "            print('Pruning ...')\n",
    "\n",
    "            gpts[name].fasterprune(ratios[k], prune_n=prune_n, prune_m=prune_m, percdamp=0.01, blocksize=128)\n",
    "            gpts[name].free()\n",
    "            k+=1\n",
    "        \n",
    "        if i == 8:\n",
    "            new_W = gpts['self_attn.v_proj'].layer.weight.data\n",
    "            scaler_row = gpts['self_attn.v_proj'].scaler_row\n",
    "            # np.save('/h3cstore_ns/jcxie/LISA/wanda-main/npys/test_admm/new_W.npy', new_W.cpu().numpy())\n",
    "            break\n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            if attention_mask is None:\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), position_ids=position_ids)[0]\n",
    "            else:\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n",
    "\n",
    "        layers[i] = layer \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c532d5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_inps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_inps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(test_outs)\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
     ]
    }
   ],
   "source": [
    "test_inps = torch.stack(test_inps)\n",
    "test_outs = torch.stack(test_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3a66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/90/8_self_attn.v_proj_test_inps.npy', test_inps.cpu().numpy())\n",
    "print(1)\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/90/8_self_attn.v_proj_test_outs.npy', test_outs.cpu().numpy())\n",
    "print(2)\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/90/8_self_attn.v_proj_ora_W.npy', ora_W.cpu().numpy())\n",
    "print(3)\n",
    "np.save('/h3cstore_ns/jcxie/LISA/nips2024/test_npy/90/8_self_attn.v_proj_new_W.npy', new_W.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
